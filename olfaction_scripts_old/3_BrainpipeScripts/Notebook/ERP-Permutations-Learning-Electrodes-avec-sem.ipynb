{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate ERPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing files and modules\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "%matplotlib notebook\n",
    "from brainpipe.system import study\n",
    "from brainpipe.visual import *\n",
    "from brainpipe.statistics import *\n",
    "from mne.baseline import rescale\n",
    "from mne.filter import filter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_pass_filter = 10.\n",
    "sf = 512.\n",
    "norm_mode = 'mean' #'ratio' 'mean' 'percent' \n",
    "baseline = [717 , 768]\n",
    "data_to_use = [768, 1536]\n",
    "n_perm = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ERPs and Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st = study('Olfacto')\n",
    "path_data = path.join (st.path, 'database/TS_E_all_cond_by_block_trigs_th40_art400_30_250_learningOK/')\n",
    "elec = 32\n",
    "\n",
    "# file to compute\n",
    "filename = 'CHAF_E1E2_concat_learning_0_bipo.npz'\n",
    "data_all = np.load(path.join(path_data, filename))\n",
    "data, channel, label = data_all['x'], data_all['channel'], data_all['label']\n",
    "\n",
    "# Select data for one elec + name :\n",
    "data_elec = data[elec,:,:]\n",
    "ntrials = len(data_elec[2])\n",
    "print ('\\nOriginal data : ', data.shape, 'Channel : ', channel[elec], 'Label : ', label[elec], 'N_trials :', ntrials, 'One elec shape : ', data_elec.shape)\n",
    "\n",
    "#Filter data for one elec (all trials):\n",
    "data = np.array(data_elec, dtype='float64')\n",
    "data_to_filter = np.swapaxes(data, 0, 1)\n",
    "filtered_data = filter_data(data_to_filter, sfreq=512, l_freq=None, h_freq=low_pass_filter, method='fir', phase='zero-double')\n",
    "filtered_data = np.swapaxes(filtered_data, 0, 1)\n",
    "print ('Size of filtered data:', filtered_data.shape,)\n",
    "\n",
    "#Normalize the non-averaged data (all trials)\n",
    "times = np.arange(filtered_data.shape[0])\n",
    "print ('time points : ', times.shape)\n",
    "filtered_data_to_norm = np.swapaxes(filtered_data, 0, 1)\n",
    "norm_filtered_data = rescale(filtered_data_to_norm, times=times, baseline=baseline, mode=norm_mode)\n",
    "norm_filtered_data = np.swapaxes(norm_filtered_data, 0, 1)\n",
    "print ('Size norm & filtered data : ', norm_filtered_data.shape,)\n",
    "\n",
    "#Generate the 3 learning files\n",
    "learning_files = np.array_split(norm_filtered_data, 3, axis=1)\n",
    "print ('learning file size 0', learning_files[0].shape, 'learning file size 1', learning_files[1].shape,'learning file size 2', learning_files[2].shape,)\n",
    "\n",
    "# =======================================  STATISTICS  =====================================\n",
    "# Range of the data to compute\n",
    "data_range = range(data_to_use[0], data_to_use[1])\n",
    "\n",
    "# Get all three learning files (Filtered data)\n",
    "data_learn_0 = learning_files[0][data_range, :]\n",
    "data_learn_1 = learning_files[1][data_range, :]\n",
    "data_learn_2 = learning_files[2][data_range, :]\n",
    "print ('-> Shape of the selected data for learn 0', data_learn_0.shape, 'learn 1', data_learn_1.shape, 'learn 2', data_learn_2.shape)\n",
    "\n",
    "# Swap data across trials (dim 1) :\n",
    "perm_learn_0_1 = perm_swap(data_learn_0, data_learn_1, axis=1, n_perm=n_perm)[0]\n",
    "perm_learn_0_2 = perm_swap(data_learn_0, data_learn_2, axis=1, n_perm=n_perm)[0]\n",
    "perm_learn_1_2 = perm_swap(data_learn_1, data_learn_2, axis=1, n_perm=n_perm)[0]\n",
    "print('-> Shape of permuted learn 0/1 : ', perm_learn_0_1.shape, 'learn 0/2 : ', perm_learn_0_2.shape, 'learn 1/2 : ', perm_learn_1_2.shape)\n",
    "\n",
    "# Take the mean across time :\n",
    "perm_learn_0_1_mean = perm_learn_0_1.mean(2)\n",
    "perm_learn_0_2_mean = perm_learn_0_2.mean(2)\n",
    "perm_learn_1_2_mean = perm_learn_1_2.mean(2)\n",
    "print('-> Shape of meaned permuted learn 0/1 : ', perm_learn_0_1_mean.shape, 'learn 0/2 : ', perm_learn_0_2_mean.shape, 'learn 1/2 : ', perm_learn_1_2_mean.shape)\n",
    "\n",
    "# Get p-values from the permuted data :\n",
    "p_vals_0_1 = perm_2pvalue(data_learn_0.mean(1), perm_learn_0_1_mean, n_perm=n_perm, threshold=None, tail=2)\n",
    "p_vals_0_2 = perm_2pvalue(data_learn_2.mean(1), perm_learn_0_2_mean, n_perm=n_perm, threshold=None, tail=2)\n",
    "p_vals_1_2 = perm_2pvalue(data_learn_1.mean(1), perm_learn_1_2_mean, n_perm=n_perm, threshold=None, tail=2)\n",
    "print('-> Shape of p-values learn 0/1 : ', p_vals_0_1.shape, 'learn 0/2 : ', p_vals_0_2.shape, 'learn 1/2 : ', p_vals_1_2.shape)\n",
    "\n",
    "## Test if there's significant p-values after multiplt comparison :\n",
    "print('-> Significant p-values learn 0/1? ', p_vals_0_1.min() <= 0.05)\n",
    "print('-> Significant p-values learn 0/2? ', p_vals_0_2.min() <= 0.05)\n",
    "print('-> Significant p-values learn 1/2? ', p_vals_1_2.min() <= 0.05)\n",
    "\n",
    "# =======================PREPARE DATA TO PLOT AND PLOT THE ERPs=====================================\n",
    "# Data to plot :\n",
    "data_learn_0_to_plot = learning_files[0][range(baseline[0], data_to_use[1])]\n",
    "data_learn_1_to_plot = learning_files[1][range(baseline[0], data_to_use[1])]\n",
    "data_learn_2_to_plot = learning_files[2][range(baseline[0], data_to_use[1])]\n",
    "data_learn_to_plot = np.concatenate([data_learn_0_to_plot, data_learn_1_to_plot, data_learn_2_to_plot], axis=1)\n",
    "print('-> Shape of filtered data to plot : ', data_learn_to_plot.shape)\n",
    "\n",
    "# Time vector & label vector:\n",
    "times_plot = 1000 * np.arange((baseline[0] - baseline[1]), data_learn_to_plot.shape[0]-baseline[1] + baseline[0],) / sf\n",
    "print('-> Shape of time vector : ', times_plot.shape)\n",
    "label_learn_0 = np.zeros(data_learn_0_to_plot[1].shape, dtype='int64')\n",
    "label_learn_1 = np.ones(data_learn_1_to_plot[1].shape, dtype='int64')\n",
    "label_learn_2 = np.full(data_learn_2_to_plot[1].shape, 2, dtype='int64')\n",
    "print('label size and values : 0 : ', label_learn_0.shape, '1 : ', label_learn_1.shape, '2 : ', label_learn_2.shape,)\n",
    "y = np.concatenate([label_learn_0, label_learn_1, label_learn_2])\n",
    "print('-> the y label', y.shape)\n",
    "\n",
    "# P-values to plot :\n",
    "p_vals_0_1_to_plot = np.insert(p_vals_0_1, 0, 10 * np.ones((baseline[1] - baseline[0],)))\n",
    "p_vals_0_2_to_plot = np.insert(p_vals_0_2, 0, 10 * np.ones((baseline[1] - baseline[0],)))\n",
    "p_vals_1_2_to_plot = np.insert(p_vals_1_2, 0, 10 * np.ones((baseline[1] - baseline[0],)))\n",
    "print('-> Shape of p-values to plot :', p_vals_0_1_to_plot.shape, p_vals_0_2_to_plot.shape, p_vals_1_2_to_plot.shape)\n",
    "\n",
    "#Prepare the plot\n",
    "fig = plt.figure(0, figsize=(12, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "ax.set_xlabel('Times (ms)', fontsize=12)\n",
    "ax.set_ylabel('Potential', fontsize=12)\n",
    "\n",
    "#Plot the Data\n",
    "BorderPlot(times_plot, data_learn_to_plot, y=y, kind='sem', alpha=0.2, color=['m','c', 'b'], linewidth=2, ncol=1, legend= ['Early Learning', 'Medium Learning', 'Late Learning'],\n",
    "          title='CHAF_ERP_Odor_bipo_'+norm_mode+'_'+str(channel[elec])+'_'+str(label[elec])+' elec_num: '+str(elec)+'_ntrials:'+str(ntrials),)\n",
    "plt.gca()\n",
    "lines = [0] #time vector is in ms\n",
    "addPval(plt.gca(), p_vals_0_1_to_plot, p=0.05, x=times_plot, y=0.5, color='m', lw=3)\n",
    "addPval(plt.gca(), p_vals_0_2_to_plot, p=0.05, x=times_plot, y=1, color='b', lw=3)\n",
    "addPval(plt.gca(), p_vals_1_2_to_plot, p=0.05, x=times_plot, y=1.5, color='c', lw=3)\n",
    "addLines(plt.gca(), vLines=lines, vColor=['firebrick'], vWidth=[2], hLines=[0], hColor=['#000000'], hWidth=[2])\n",
    "plt.legend(fontsize='small')\n",
    "plt.grid()\n",
    "plt.show()         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot all ERPs with stats for learning cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = study('Olfacto')\n",
    "path_data = path.join (st.path, 'database/TS_E_all_cond_by_block_trigs_th40_art400_30_250/')\n",
    "\n",
    "subjects = ['CHAF','VACJ','SEMC','PIRJ','LEFC','MICP',]\n",
    "\n",
    "n_elec = {\n",
    "    'CHAF' : 107,\n",
    "    'VACJ' : 139, \n",
    "    'SEMC' : 107,\n",
    "    'PIRJ' : 106,\n",
    "    'LEFC' : 193,\n",
    "    'MICP' : 105,\n",
    "}\n",
    "\n",
    "for su in subjects:\n",
    "    for elec in range(0, n_elec[su],1):\n",
    "        filename = su+'_E1E2_concat_allfilter1.npz'\n",
    "        print (filename)\n",
    "        data_all = np.load(path.join(path_data, filename))\n",
    "        #data, channel, label = data_all['x'], data_all['channel'], data_all['label']\n",
    "        data, channel = data_all['x'], [data_all['channel'][i][0] for i in range(len(data_all['channel']))]\n",
    "        label = [data_all['label'][i][0] for i in range(len(data_all['label']))]\n",
    "\n",
    "        # Select data for one elec + name :\n",
    "        data_elec = data[elec,:,:]\n",
    "        ntrials = len(data_elec[2])\n",
    "        print ('\\nOriginal data : ', data.shape, 'Channel : ', channel[elec], 'Label : ', label[elec], 'N_trials :', ntrials, 'One elec shape : ', data_elec.shape)\n",
    "\n",
    "        #Filter data for one elec (all trials):\n",
    "        data = np.array(data_elec, dtype='float64')\n",
    "        data_to_filter = np.swapaxes(data, 0, 1)\n",
    "        filtered_data = filter_data(data_to_filter, sfreq=512, l_freq=None, h_freq=low_pass_filter, method='fir', phase='zero-double')\n",
    "        filtered_data = np.swapaxes(filtered_data, 0, 1)\n",
    "        print ('Size of filtered data:', filtered_data.shape,)\n",
    "\n",
    "        #Normalize the non-averaged data (all trials)\n",
    "        times = np.arange(filtered_data.shape[0])\n",
    "        print ('time points : ', times.shape)\n",
    "        filtered_data_to_norm = np.swapaxes(filtered_data, 0, 1)\n",
    "        norm_filtered_data = rescale(filtered_data_to_norm, times=times, baseline=baseline, mode=norm_mode)\n",
    "        norm_filtered_data = np.swapaxes(norm_filtered_data, 0, 1)\n",
    "        print ('Size norm & filtered data : ', norm_filtered_data.shape,)\n",
    "\n",
    "        #Generate the 3 learning files\n",
    "        learning_files = np.array_split(norm_filtered_data, 3, axis=1)\n",
    "        print ('learning file size 0', learning_files[0].shape, 'learning file size 1', learning_files[1].shape,'learning file size 2', learning_files[2].shape,)\n",
    "\n",
    "# =======================================  STATISTICS  =====================================\n",
    "        # Range of the data to compute\n",
    "        data_range = range(data_to_use[0], data_to_use[1])\n",
    "\n",
    "        # Get all three learning files (Filtered data)\n",
    "        data_learn_0 = learning_files[0][data_range, :]\n",
    "        data_learn_1 = learning_files[1][data_range, :]\n",
    "        data_learn_2 = learning_files[2][data_range, :]\n",
    "        print ('-> Shape of the selected data for learn 0', data_learn_0.shape, 'learn 1', data_learn_1.shape, 'learn 2', data_learn_2.shape)\n",
    "\n",
    "        # Swap data across trials (dim 1) :\n",
    "        perm_learn_0_1 = perm_swap(data_learn_0, data_learn_1, axis=1, n_perm=n_perm)[0]\n",
    "        perm_learn_0_2 = perm_swap(data_learn_0, data_learn_2, axis=1, n_perm=n_perm)[0]\n",
    "        perm_learn_1_2 = perm_swap(data_learn_1, data_learn_2, axis=1, n_perm=n_perm)[0]\n",
    "        print('-> Shape of permuted learn 0/1 : ', perm_learn_0_1.shape, 'learn 0/2 : ', perm_learn_0_2.shape, 'learn 1/2 : ', perm_learn_1_2.shape)\n",
    "\n",
    "        # Take the mean across time :\n",
    "        perm_learn_0_1_mean = perm_learn_0_1.mean(2)\n",
    "        perm_learn_0_2_mean = perm_learn_0_2.mean(2)\n",
    "        perm_learn_1_2_mean = perm_learn_1_2.mean(2)\n",
    "        print('-> Shape of meaned permuted learn 0/1 : ', perm_learn_0_1_mean.shape, 'learn 0/2 : ', perm_learn_0_2_mean.shape, 'learn 1/2 : ', perm_learn_1_2_mean.shape)\n",
    "\n",
    "        # Get p-values from the permuted data :\n",
    "        p_vals_0_1 = perm_2pvalue(data_learn_0.mean(1), perm_learn_0_1_mean, n_perm=n_perm, threshold=None, tail=2)\n",
    "        p_vals_0_2 = perm_2pvalue(data_learn_2.mean(1), perm_learn_0_2_mean, n_perm=n_perm, threshold=None, tail=2)\n",
    "        p_vals_1_2 = perm_2pvalue(data_learn_1.mean(1), perm_learn_1_2_mean, n_perm=n_perm, threshold=None, tail=2)\n",
    "        print('-> Shape of p-values learn 0/1 : ', p_vals_0_1.shape, 'learn 0/2 : ', p_vals_0_2.shape, 'learn 1/2 : ', p_vals_1_2.shape)\n",
    "\n",
    "        ## Test if there's significant p-values after multiplt comparison :\n",
    "        print('-> Significant p-values learn 0/1? ', p_vals_0_1.min() <= 0.05)\n",
    "        print('-> Significant p-values learn 0/2? ', p_vals_0_2.min() <= 0.05)\n",
    "        print('-> Significant p-values learn 1/2? ', p_vals_1_2.min() <= 0.05)\n",
    "\n",
    "        # =======================PREPARE DATA TO PLOT AND PLOT THE ERPs=====================================\n",
    "        # Data to plot :\n",
    "        data_learn_0_to_plot = learning_files[0][range(baseline[0], data_to_use[1])]\n",
    "        data_learn_1_to_plot = learning_files[1][range(baseline[0], data_to_use[1])]\n",
    "        data_learn_2_to_plot = learning_files[2][range(baseline[0], data_to_use[1])]\n",
    "        data_learn_to_plot = np.concatenate([data_learn_0_to_plot, data_learn_1_to_plot, data_learn_2_to_plot], axis=1)\n",
    "        print('-> Shape of filtered data to plot : ', data_learn_to_plot.shape)\n",
    "\n",
    "        # Time vector & label vector:\n",
    "        times_plot = 1000 * np.arange((baseline[0] - baseline[1]), data_learn_to_plot.shape[0]-baseline[1] + baseline[0],) / sf\n",
    "        print('-> Shape of time vector : ', times_plot.shape)\n",
    "        label_learn_0 = np.zeros(data_learn_0_to_plot[1].shape, dtype='int64')\n",
    "        label_learn_1 = np.ones(data_learn_1_to_plot[1].shape, dtype='int64')\n",
    "        label_learn_2 = np.full(data_learn_2_to_plot[1].shape, 2, dtype='int64')\n",
    "        print('label size and values : 0 : ', label_learn_0.shape, '1 : ', label_learn_1.shape, '2 : ', label_learn_2.shape,)\n",
    "        y = np.concatenate([label_learn_0, label_learn_1, label_learn_2])\n",
    "        print('-> the y label', y.shape)\n",
    "\n",
    "        # P-values to plot :\n",
    "        p_vals_0_1_to_plot = np.insert(p_vals_0_1, 0, 10 * np.ones((baseline[1] - baseline[0],)))\n",
    "        p_vals_0_2_to_plot = np.insert(p_vals_0_2, 0, 10 * np.ones((baseline[1] - baseline[0],)))\n",
    "        p_vals_1_2_to_plot = np.insert(p_vals_1_2, 0, 10 * np.ones((baseline[1] - baseline[0],)))\n",
    "        print('-> Shape of p-values to plot :', p_vals_0_1_to_plot.shape, p_vals_0_2_to_plot.shape, p_vals_1_2_to_plot.shape)\n",
    "\n",
    "        #Prepare the plot\n",
    "        fig = plt.figure(0, figsize=(12, 7))\n",
    "        ax = fig.add_subplot(111)\n",
    "        fig.subplots_adjust(top=0.85)\n",
    "        ax.set_xlabel('Times (ms)', fontsize=12)\n",
    "        ax.set_ylabel('Potential', fontsize=12)\n",
    "\n",
    "        #Plot the Data\n",
    "        BorderPlot(times_plot, data_learn_to_plot, y=y, kind='sem', alpha=0.2, color=['m','c', 'b'], linewidth=2, ncol=1, legend= ['Early Learning', 'Medium Learning', 'Late Learning'],\n",
    "                  title=su+'_ERP_Odor_mono_'+norm_mode+'_'+str(channel[elec])+'_'+str(label[elec])+' elec_num: '+str(elec)+'_ntrials:'+str(ntrials),)\n",
    "        plt.gca()\n",
    "        lines = [0] #time vector is in ms\n",
    "        addPval(plt.gca(), p_vals_0_1_to_plot, p=0.05, x=times_plot, y=0.5, color='m', lw=3)\n",
    "        addPval(plt.gca(), p_vals_0_2_to_plot, p=0.05, x=times_plot, y=1, color='b', lw=3)\n",
    "        addPval(plt.gca(), p_vals_1_2_to_plot, p=0.05, x=times_plot, y=1.5, color='c', lw=3)\n",
    "        addLines(plt.gca(), vLines=lines, vColor=['firebrick'], vWidth=[2], hLines=[0], hColor=['#000000'], hWidth=[2])\n",
    "        plt.legend(fontsize='small')\n",
    "        plt.grid()\n",
    "        #plt.show()     \n",
    "\n",
    "# =========================SAVE PLOTS of ERPs=================================================\n",
    "        rep = path.join(st.path, 'feature/ERP_Encoding_all_mono_100ms_mean_thr40_art400_30_250_learning/',su)\n",
    "        fname = (rep + '_E1E2_ERP_concat_all_mono_' + channel [elec] +'_'+str(elec)+'_'+label[elec]+'.png')\n",
    "        print (fname)\n",
    "        plt.savefig(fname, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "del x, channel, n_elec, n_trials, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
