{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate ERPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing files and modules\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "#%matplotlib notebook\n",
    "from brainpipe.system import study\n",
    "from brainpipe.visual import *\n",
    "from brainpipe.statistics import *\n",
    "from mne.baseline import rescale\n",
    "from mne.filter import filter_data\n",
    "from mne.stats import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_pass_filter = 10.\n",
    "sf = 512.\n",
    "norm_mode = 'mean' #'ratio' 'mean' 'percent' \n",
    "baseline = [896 , 1024]\n",
    "data_to_use = [896, 1536]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ERPs and Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = study('Olfacto')\n",
    "path_data = path.join (st.path, 'database/TS_E_all_by_odor_th40_art400_30_250_5s_concatOK/')\n",
    "elec = 84\n",
    "\n",
    "#Load files\n",
    "name_early = 'PIRJ_concat_odor_bad_bipo.npz'\n",
    "name_late = 'PIRJ_concat_odor_good_bipo.npz'\n",
    "data_early = np.load(path.join(path_data, name_early))\n",
    "data_late = np.load(path.join(path_data, name_late))\n",
    "data_early, channel, label, data_late = data_early['x'], data_early['channel'], data_early['label'], data_late['x']\n",
    "\n",
    "# Select data for one elec + name :\n",
    "data_elec_early = data_early[elec,:,:]\n",
    "data_elec_late = data_late[elec,:,:]\n",
    "ntrials = len(data_elec_early[2])\n",
    "print ('Channel : ', channel[elec], 'Label : ', label[elec], 'N_trials :', ntrials, \n",
    "       'One elec shape : ', data_elec_early.shape)\n",
    "\n",
    "#Filter data for one elec (all trials):\n",
    "data_elec_early = np.array(data_elec_early, dtype='float64')\n",
    "data_elec_late = np.array(data_elec_late, dtype='float64')\n",
    "data_early_to_filter = np.swapaxes(data_elec_early, 0, 1)\n",
    "data_late_to_filter = np.swapaxes(data_elec_late, 0, 1)\n",
    "filtered_data_early = filter_data(data_early_to_filter, sfreq=512, l_freq=None, h_freq=low_pass_filter, method='fir', phase='zero-double')\n",
    "filtered_data_late = filter_data(data_late_to_filter, sfreq=512, l_freq=None, h_freq=low_pass_filter, method='fir', phase='zero-double')\n",
    "print ('Size of filtered data early :', filtered_data_early.shape, 'filtered data late : ', filtered_data_late.shape,)\n",
    "\n",
    "#Normalize the non-averaged data (all trials)\n",
    "times = np.arange(filtered_data_early.shape[1])\n",
    "print ('time points : ', times.shape)\n",
    "norm_filtered_data_early = rescale(filtered_data_early, times=times, baseline=baseline, mode=norm_mode)\n",
    "norm_filtered_data_late = rescale(filtered_data_late, times=times, baseline=baseline, mode=norm_mode)\n",
    "print ('Size norm & filtered data 0 : ', norm_filtered_data_early.shape, norm_filtered_data_late.shape,)\n",
    "\n",
    "# =======================================  STATISTICS  =====================================\n",
    "# Range of the data to compute\n",
    "data_range = range(data_to_use[0], data_to_use[1])\n",
    "\n",
    "# Get all three learning files (Filtered data)\n",
    "data_learn_0 = norm_filtered_data_early[:, data_range]\n",
    "data_learn_1 = norm_filtered_data_late[:, data_range,]\n",
    "print ('-> Shape of the selected data for learn 0', data_learn_0.shape, 'learn 1', data_learn_1.shape,)\n",
    "\n",
    "#reshape data to have the exact same nb od trials (mandatory for t-tests)\n",
    "data_learn_0_rand = data_learn_0[np.random.randint(data_learn_0.shape[0], size=data_learn_1.shape[0]), :] #reshape early_data to fit late_data shape\n",
    "print ('rand early matrix', data_learn_0_rand.shape)\n",
    "X = data_learn_0_rand - data_learn_1 #the last dimension needs to be time\n",
    "T0, p_values, H0 = permutation_t_test(X, n_permutations=1000, tail=0, n_jobs=1, verbose=None)\n",
    "\n",
    "# =======================PREPARE DATA TO PLOT AND PLOT THE ERPs=====================================\n",
    "# Data to plot :\n",
    "data_learn_to_plot = np.concatenate([data_learn_0_rand, data_learn_1,], axis=0)\n",
    "data_learn_to_plot = data_learn_to_plot.swapaxes(0,1)\n",
    "print('-> Shape of data to plot : ', data_learn_to_plot.shape)\n",
    "\n",
    "# Time vector & label vector:\n",
    "times_plot = 1000 * np.arange((baseline[0] - baseline[1]), data_learn_to_plot.shape[0]-baseline[1] + baseline[0],) / sf\n",
    "print('-> Shape of time vector : ', times_plot.shape)\n",
    "label_learn_0 = np.zeros(data_learn_0_rand.shape[0], dtype='int64')\n",
    "label_learn_1 = np.ones(data_learn_1.shape[0], dtype='int64')\n",
    "print('label size and values : 0 : ', label_learn_0.shape, '1 : ', label_learn_1.shape,)\n",
    "y = np.concatenate([label_learn_0, label_learn_1,])\n",
    "print('-> the y label', y.shape)\n",
    "\n",
    "#Prepare the plot\n",
    "fig = plt.figure(0, figsize=(12, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "ax.set_xlabel('Times (ms)', fontsize=12)\n",
    "ax.set_ylabel('Potential', fontsize=12)\n",
    "\n",
    "#Plot the Data\n",
    "BorderPlot(times_plot, data_learn_to_plot, y=y, kind='sem', alpha=0.2, color=['m','b'], linewidth=2, ncol=1, legend= ['Early Learning', 'Late Learning'],\n",
    "          title='PIRJ_ERP_Odor_bipo_'+norm_mode+'_'+str(channel[elec])+'_'+str(label[elec])+' elec_num: '+str(elec)+'_ntrials:'+str(ntrials),)\n",
    "plt.gca()\n",
    "lines = [0] #time vector is in ms\n",
    "addPval(plt.gca(), p_values, p=0.05, x=times_plot, y=0.5, color='m', lw=3)\n",
    "addLines(plt.gca(), vLines=lines, vColor=['firebrick'], vWidth=[2], hLines=[0], hColor=['#000000'], hWidth=[2])\n",
    "plt.legend(fontsize='small')\n",
    "plt.grid()\n",
    "plt.show()         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute ERPs for odor groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = study('Olfacto')\n",
    "path_data = path.join (st.path, 'database/TS_E_all_by_odor_th40_art400_30_250_5s_concatOK/')\n",
    "elec = 26\n",
    "\n",
    "#Load files\n",
    "badname = 'PIRJ_concat_odor_bad_bipo.npz'\n",
    "goodname = 'PIRJ_concat_odor_good_bipo.npz'\n",
    "data_bad = np.load(path.join(path_data, badname))\n",
    "data_good = np.load(path.join(path_data, goodname))\n",
    "data_bad, channel, label, data_good = data_bad['x'], data_bad['channel'], data_bad['label'], data_good['x']\n",
    "print (data_bad.shape, data_good.shape)\n",
    "\n",
    "# Select data for one elec + name :\n",
    "data_elec_bad = data_bad[elec,:,:]\n",
    "data_elec_good = data_good[elec,:,:]\n",
    "ntrials = len(data_elec_bad[2])\n",
    "print ('Channel : ', channel[elec], 'Label : ', label[elec], 'N_trials :', ntrials, \n",
    "       'Bad shape : ', data_elec_bad.shape, 'Good shape : ', data_elec_good.shape)\n",
    "\n",
    "#Filter data for one elec (all trials):\n",
    "data_elec_bad = np.array(data_elec_bad, dtype='float64')\n",
    "data_elec_good = np.array(data_elec_good, dtype='float64')\n",
    "data_bad_to_filter = np.swapaxes(data_elec_bad, 0, 1)\n",
    "data_good_to_filter = np.swapaxes(data_elec_good, 0, 1)\n",
    "filtered_data_bad = filter_data(data_bad_to_filter, sfreq=512, l_freq=None, h_freq=low_pass_filter, method='fir', phase='zero-double')\n",
    "filtered_data_good = filter_data(data_good_to_filter, sfreq=512, l_freq=None, h_freq=low_pass_filter, method='fir', phase='zero-double')\n",
    "print ('Size of filtered data bad :', filtered_data_bad.shape, 'filtered data good : ', filtered_data_good.shape,)\n",
    "\n",
    "#Normalize the non-averaged data (all trials)\n",
    "times = np.arange(filtered_data_bad.shape[1])\n",
    "print ('time points : ', times.shape)\n",
    "norm_filtered_data_bad = rescale(filtered_data_bad, times=times, baseline=baseline, mode=norm_mode)\n",
    "norm_filtered_data_good = rescale(filtered_data_good, times=times, baseline=baseline, mode=norm_mode)\n",
    "print ('Size norm & filtered data 0 : ', norm_filtered_data_bad.shape, norm_filtered_data_good.shape,)\n",
    "\n",
    "# =======================================  STATISTICS  =====================================\n",
    "# Range of the data to compute\n",
    "data_range = range(data_to_use[0], data_to_use[1])\n",
    "\n",
    "# Get all three learning files (Filtered data)\n",
    "data_bad = norm_filtered_data_bad[:, data_range]\n",
    "data_good = norm_filtered_data_good[:, data_range,]\n",
    "print ('-> Shape of the selected data for learn 0', data_bad.shape, 'learn 1', data_good.shape,)\n",
    "\n",
    "# #reshape data to have the exact same nb od trials (mandatory for t-tests)\n",
    "# if data_bad.shape[0] > data_good.shape[0]:\n",
    "#     data_bad_stats = data_bad[np.random.randint(data_bad.shape[0], size=data_good.shape[0]), :] #reshape bad_data to fit good_data shape\n",
    "#     data_good_stats = data_good\n",
    "#     print ('rand bad matrix', data_bad.shape)\n",
    "# if data_bad.shape[0] < data_good.shape[0]:\n",
    "#     data_good_rand = data_good[np.random.randint(data_good.shape[0], size=data_bad.shape[0]), :]\n",
    "#     data_bad_stats = data_bad\n",
    "#     print ('rand good matrix', data_good.shape)\n",
    "# X = data_bad_stats - data_good_stats #the last dimension needs to be time\n",
    "# T0, p_values, H0 = permutation_t_test(X, n_permutations=1000, tail=0, n_jobs=1, verbose=None)\n",
    "\n",
    "# =======================PREPARE DATA TO PLOT AND PLOT THE ERPs=====================================\n",
    "# Data to plot :\n",
    "data_learn_to_plot = np.concatenate([data_bad, data_good,], axis=0)\n",
    "data_learn_to_plot = data_learn_to_plot.swapaxes(0,1)\n",
    "print('-> Shape of data to plot : ', data_learn_to_plot.shape)\n",
    "\n",
    "# Time vector & label vector:\n",
    "times_plot = 1000 * np.arange((baseline[0] - baseline[1]), data_learn_to_plot.shape[0]-baseline[1] + baseline[0],) / sf\n",
    "print('-> Shape of time vector : ', times_plot.shape)\n",
    "label_bad = np.zeros(data_bad.shape[0], dtype='int64')\n",
    "label_good = np.ones(data_good.shape[0], dtype='int64')\n",
    "print('label size and values : 0 : ', label_bad.shape, '1 : ', label_good.shape,)\n",
    "y = np.concatenate([label_bad, label_good,])\n",
    "print('-> the y label', y.shape)\n",
    "\n",
    "#Prepare the plot\n",
    "fig = plt.figure(0, figsize=(12, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "ax.set_xlabel('Times (ms)', fontsize=12)\n",
    "ax.set_ylabel('Potential', fontsize=12)\n",
    "\n",
    "#Plot the Data\n",
    "BorderPlot(times_plot, data_learn_to_plot, y=y, kind='sem', alpha=0.2, color=['m','b'], linewidth=2, ncol=1, legend= ['bad group', 'good group'],\n",
    "          title='PIRJ_ERP_Odor_bipo_'+norm_mode+'_'+str(channel[elec])+'_'+str(label[elec])+' elec_num: '+str(elec)+'_ntrials:'+str(ntrials),)\n",
    "plt.gca()\n",
    "lines = [0] #time vector is in ms\n",
    "addPval(plt.gca(), p_values, p=0.05, x=times_plot, y=0.5, color='m', lw=3)\n",
    "addLines(plt.gca(), vLines=lines, vColor=['firebrick'], vWidth=[2], hLines=[0], hColor=['#000000'], hWidth=[2])\n",
    "plt.legend(fontsize='small')\n",
    "plt.grid()\n",
    "plt.show()         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot all ERPs with stats for learning cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = study('Olfacto')\n",
    "path_data = path.join (st.path, 'database/TS_E_all_cond_by_block_trigs_th40_art400_30_250_5s_learning2blocks/')\n",
    "subjects = ['SEMC','PIRJ','LEFC','MICP','CHAF','VACJ',] \n",
    "nelec=10\n",
    "\n",
    "for su in subjects:\n",
    "    for elec in range(nelec):\n",
    "        name_early = su+'_E1E2_concat_early_bipo.npz'\n",
    "        data_early = np.load(path.join(path_data, name_early))\n",
    "        data_early = data_early['channel'][elec]\n",
    "        print (su, data_early)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "st = study('Olfacto')\n",
    "path_data = path.join (st.path, 'database/TS_E_all_cond_by_block_trigs_th40_art400_30_250_5s_learning2blocks/')\n",
    "\n",
    "subjects = ['SEMC','PIRJ','LEFC','MICP','CHAF','VACJ',] \n",
    "\n",
    "n_elec = {\n",
    "    'CHAF' : 107,\n",
    "    'VACJ' : 139, \n",
    "    'SEMC' : 107,\n",
    "    'PIRJ' : 106,\n",
    "    'LEFC' : 193,\n",
    "    'MICP' : 105,\n",
    "}\n",
    "\n",
    "\n",
    "for su in subjects:\n",
    "    for elec in range(0, n_elec[su],1):\n",
    "        #Load files\n",
    "        name_early = su+'_E1E2_concat_early_bipo.npz'\n",
    "        name_late = su+'_E1E2_concat_late_bipo.npz'\n",
    "        data_early = np.load(path.join(path_data, name_early))\n",
    "        data_late = np.load(path.join(path_data, name_late))\n",
    "        data_early, channel, label, data_late = data_early['x'], data_early['channel'], data_early['label'], data_late['x']\n",
    "\n",
    "        # Select data for one elec + name :\n",
    "        data_elec_early = data_early[elec,:,:]\n",
    "        data_elec_late = data_late[elec,:,:]\n",
    "        ntrials = len(data_elec_early[2])\n",
    "        print ('Channel : ', channel[elec], 'Label : ', label[elec], 'N_trials :', ntrials, 'One elec shape : ', data_elec_early.shape)\n",
    "\n",
    "        #Filter data for one elec (all trials):\n",
    "        data_elec_early = np.array(data_elec_early, dtype='float64')\n",
    "        data_elec_late = np.array(data_elec_late, dtype='float64')\n",
    "        data_early_to_filter = np.swapaxes(data_elec_early, 0, 1)\n",
    "        data_late_to_filter = np.swapaxes(data_elec_late, 0, 1)\n",
    "        filtered_data_early = filter_data(data_early_to_filter, sfreq=512, l_freq=None, h_freq=low_pass_filter, method='fir', phase='zero-double')\n",
    "        filtered_data_late = filter_data(data_late_to_filter, sfreq=512, l_freq=None, h_freq=low_pass_filter, method='fir', phase='zero-double')\n",
    "        print ('Size of filtered data early :', filtered_data_early.shape, 'filtered data late : ', filtered_data_late.shape,)     \n",
    "\n",
    "        #Normalize the non-averaged data (all trials)\n",
    "        times = np.arange(filtered_data_early.shape[1])\n",
    "        print ('time points : ', times.shape)\n",
    "        norm_filtered_data_early = rescale(filtered_data_early, times=times, baseline=baseline, mode=norm_mode)\n",
    "        norm_filtered_data_late = rescale(filtered_data_late, times=times, baseline=baseline, mode=norm_mode)\n",
    "        print ('Size norm & filtered data 0 : ', norm_filtered_data_early.shape, norm_filtered_data_late.shape,)\n",
    "\n",
    "        # =======================================  STATISTICS  =====================================\n",
    "        # Range of the data to compute\n",
    "        data_range = range(data_to_use[0], data_to_use[1])\n",
    "\n",
    "        # Get all three learning files (Filtered data)\n",
    "        data_learn_0 = norm_filtered_data_early[:, data_range]\n",
    "        data_learn_1 = norm_filtered_data_late[:, data_range,]\n",
    "        print ('-> Shape of the selected data for learn 0', data_learn_0.shape, 'learn 1', data_learn_1.shape,)\n",
    "\n",
    "        #reshape data to have the exact same nb od trials (mandatory for t-tests)\n",
    "        data_learn_0_rand = data_learn_0[np.random.randint(data_learn_0.shape[0], size=data_learn_1.shape[0]), :] #reshape early_data to fit late_data shape\n",
    "        print ('rand early matrix', data_learn_0_rand.shape)\n",
    "        X = data_learn_0_rand - data_learn_1 #the last dimension needs to be time\n",
    "        T0, p_values, H0 = permutation_t_test(X, n_permutations=1000, tail=0, n_jobs=1, verbose=None)\n",
    "\n",
    "        # =======================PREPARE DATA TO PLOT AND PLOT THE ERPs=====================================            \n",
    "        #if p_values.min() <= 0.05:\n",
    "        # Data to plot :\n",
    "        data_learn_to_plot = np.concatenate([data_learn_0_rand, data_learn_1,], axis=0)\n",
    "        data_learn_to_plot = data_learn_to_plot.swapaxes(0,1)\n",
    "        print('-> Shape of data to plot : ', data_learn_to_plot.shape)\n",
    "\n",
    "        # Time vector & label vector:\n",
    "        times_plot = 1000 * np.arange((baseline[0] - baseline[1]), data_learn_to_plot.shape[0]-baseline[1] + baseline[0],) / sf\n",
    "        print('-> Shape of time vector : ', times_plot.shape)\n",
    "        label_learn_0 = np.zeros(data_learn_0_rand.shape[0], dtype='int64')\n",
    "        label_learn_1 = np.ones(data_learn_1.shape[0], dtype='int64')\n",
    "        print('label size and values : 0 : ', label_learn_0.shape, '1 : ', label_learn_1.shape,)\n",
    "        y = np.concatenate([label_learn_0, label_learn_1,])\n",
    "        print('-> the y label', y.shape)\n",
    "\n",
    "        #Prepare the plot\n",
    "        fig = plt.figure(0, figsize=(12, 7))\n",
    "        ax = fig.add_subplot(111)\n",
    "        fig.subplots_adjust(top=0.85)\n",
    "        ax.set_xlabel('Times (ms)', fontsize=12)\n",
    "        ax.set_ylabel('Potential', fontsize=12)\n",
    "\n",
    "        #Plot the Data\n",
    "        BorderPlot(times_plot, data_learn_to_plot, y=y, kind='sem', alpha=0.2, color=['m', 'b'], linewidth=2, ncol=1, legend= ['Early Learning', 'Late Learning'],\n",
    "                  title=su+'_ERP_Odor_bipo_'+norm_mode+'_'+str(channel[elec])+'_'+str(label[elec])+' elec_num: '+str(elec)+'_ntrials:'+str(ntrials),)\n",
    "        plt.gca()\n",
    "        lines = [0] #time vector is in ms\n",
    "        addPval(plt.gca(), p_values, p=0.05, x=times_plot, y=2, color='c', lw=3)\n",
    "        addLines(plt.gca(), vLines=lines, vColor=['firebrick'], vWidth=[2], hLines=[0], hColor=['#000000'], hWidth=[2])\n",
    "        plt.legend(fontsize='small')\n",
    "        plt.grid()\n",
    "        #plt.show()         \n",
    "\n",
    "# =========================  SAVE PLOTS of ERPs   =================================================\n",
    "        rep = path.join(st.path, 'feature/ERP_Encoding_all_bipo_250ms_mean_thr40_art400_30_250_learning_2blocks/',su)\n",
    "        fname = (rep + '_E1E2_ERP_concat_all_bipo_' + channel [elec] +'_'+str(elec)+'_'+label[elec]+'.png')\n",
    "        print (fname)\n",
    "        plt.savefig(fname, dpi=300, bbox_inches='tight')\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        del channel, ntrials, label, data_learn_0_rand, data_learn_0, data_learn_1,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ERPs for Odor groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Olfacto loaded\n",
      "Channel :  b2-b1 Label :  aHC&aHC-Ent N_trials : 21/19 Bad shape :  (2560, 21) Good shape :  (2560, 19)\n",
      "Setting up low-pass filter at 10 Hz\n",
      "h_trans_bandwidth chosen to be 2.5 Hz\n",
      "Filter length of 1352 samples (2.641 sec) selected\n",
      "Setting up low-pass filter at 10 Hz\n",
      "h_trans_bandwidth chosen to be 2.5 Hz\n",
      "Filter length of 1352 samples (2.641 sec) selected\n",
      "Size of filtered data bad : (21, 2560) filtered data good :  (19, 2560)\n",
      "time points :  (2560,)\n",
      "Applying baseline correction (mode: mean)\n",
      "Applying baseline correction (mode: mean)\n",
      "Size norm & filtered data 0 :  (21, 2560) (19, 2560)\n",
      "-> Shape of the selected data for learn 0 (21, 640) learn 1 (19, 640)\n",
      "rand bad matrix (19, 640)\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Channel :  b3-b2 Label :  aHC N_trials : 21/19 Bad shape :  (2560, 21) Good shape :  (2560, 19)\n",
      "Setting up low-pass filter at 10 Hz\n",
      "h_trans_bandwidth chosen to be 2.5 Hz\n",
      "Filter length of 1352 samples (2.641 sec) selected\n",
      "Setting up low-pass filter at 10 Hz\n",
      "h_trans_bandwidth chosen to be 2.5 Hz\n",
      "Filter length of 1352 samples (2.641 sec) selected\n",
      "Size of filtered data bad : (21, 2560) filtered data good :  (19, 2560)\n",
      "time points :  (2560,)\n",
      "Applying baseline correction (mode: mean)\n",
      "Applying baseline correction (mode: mean)\n",
      "Size norm & filtered data 0 :  (21, 2560) (19, 2560)\n",
      "-> Shape of the selected data for learn 0 (21, 640) learn 1 (19, 640)\n",
      "rand bad matrix (19, 640)\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Channel :  b4-b3 Label :  aHC N_trials : 21/19 Bad shape :  (2560, 21) Good shape :  (2560, 19)\n",
      "Setting up low-pass filter at 10 Hz\n",
      "h_trans_bandwidth chosen to be 2.5 Hz\n",
      "Filter length of 1352 samples (2.641 sec) selected\n",
      "Setting up low-pass filter at 10 Hz\n",
      "h_trans_bandwidth chosen to be 2.5 Hz\n",
      "Filter length of 1352 samples (2.641 sec) selected\n",
      "Size of filtered data bad : (21, 2560) filtered data good :  (19, 2560)\n",
      "time points :  (2560,)\n",
      "Applying baseline correction (mode: mean)\n",
      "Applying baseline correction (mode: mean)\n",
      "Size norm & filtered data 0 :  (21, 2560) (19, 2560)\n",
      "-> Shape of the selected data for learn 0 (21, 640) learn 1 (19, 640)\n",
      "rand bad matrix (19, 640)\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Channel :  b5-b4 Label :  aHC-PHG&aHC N_trials : 21/19 Bad shape :  (2560, 21) Good shape :  (2560, 19)\n",
      "Setting up low-pass filter at 10 Hz\n",
      "h_trans_bandwidth chosen to be 2.5 Hz\n",
      "Filter length of 1352 samples (2.641 sec) selected\n",
      "Setting up low-pass filter at 10 Hz\n",
      "h_trans_bandwidth chosen to be 2.5 Hz\n",
      "Filter length of 1352 samples (2.641 sec) selected\n",
      "Size of filtered data bad : (21, 2560) filtered data good :  (19, 2560)\n",
      "time points :  (2560,)\n",
      "Applying baseline correction (mode: mean)\n",
      "Applying baseline correction (mode: mean)\n",
      "Size norm & filtered data 0 :  (21, 2560) (19, 2560)\n",
      "-> Shape of the selected data for learn 0 (21, 640) learn 1 (19, 640)\n",
      "rand bad matrix (19, 640)\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Channel :  b6-b5 Label :  PHG-FuG&aHC-PHG N_trials : 21/19 Bad shape :  (2560, 21) Good shape :  (2560, 19)\n",
      "Setting up low-pass filter at 10 Hz\n",
      "h_trans_bandwidth chosen to be 2.5 Hz\n",
      "Filter length of 1352 samples (2.641 sec) selected\n",
      "Setting up low-pass filter at 10 Hz\n",
      "h_trans_bandwidth chosen to be 2.5 Hz\n",
      "Filter length of 1352 samples (2.641 sec) selected\n",
      "Size of filtered data bad : (21, 2560) filtered data good :  (19, 2560)\n",
      "time points :  (2560,)\n",
      "Applying baseline correction (mode: mean)\n",
      "Applying baseline correction (mode: mean)\n",
      "Size norm & filtered data 0 :  (21, 2560) (19, 2560)\n",
      "-> Shape of the selected data for learn 0 (21, 640) learn 1 (19, 640)\n",
      "rand bad matrix (19, 640)\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Channel :  b7-b6 Label :  WM-FuG&PHG-FuG N_trials : 21/19 Bad shape :  (2560, 21) Good shape :  (2560, 19)\n",
      "Setting up low-pass filter at 10 Hz\n",
      "h_trans_bandwidth chosen to be 2.5 Hz\n",
      "Filter length of 1352 samples (2.641 sec) selected\n",
      "Setting up low-pass filter at 10 Hz\n",
      "h_trans_bandwidth chosen to be 2.5 Hz\n",
      "Filter length of 1352 samples (2.641 sec) selected\n",
      "Size of filtered data bad : (21, 2560) filtered data good :  (19, 2560)\n",
      "time points :  (2560,)\n",
      "Applying baseline correction (mode: mean)\n",
      "Applying baseline correction (mode: mean)\n",
      "Size norm & filtered data 0 :  (21, 2560) (19, 2560)\n",
      "-> Shape of the selected data for learn 0 (21, 640) learn 1 (19, 640)\n",
      "rand bad matrix (19, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0f70a3261926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'rand good matrix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_good\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_bad\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata_good\u001b[0m \u001b[0;31m#the last dimension needs to be time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mT0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermutation_t_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_permutations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mT_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_rep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_rep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mT0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mp_val_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_val_rep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_val_rep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mp_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/karim/anaconda3/lib/python3.5/site-packages/mne-python/mne/stats/permutations.py\u001b[0m in \u001b[0;36mpermutation_t_test\u001b[0;34m(X, n_permutations, tail, n_jobs, verbose)\u001b[0m\n",
      "\u001b[0;32m/home/karim/anaconda3/lib/python3.5/site-packages/mne-python/mne/utils.py\u001b[0m in \u001b[0;36mverbose\u001b[0;34m(function, *args, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0muse_log_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/karim/anaconda3/lib/python3.5/site-packages/mne-python/mne/stats/permutations.py\u001b[0m in \u001b[0;36mpermutation_t_test\u001b[0;34m(X, n_permutations, tail, n_jobs, verbose)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     max_abs = np.concatenate(parallel(my_max_stat(X, X2, p, dof_scaling)\n\u001b[0;32m--> 137\u001b[0;31m                                       for p in np.array_split(perms, n_jobs)))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0mH0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_abs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/karim/anaconda3/lib/python3.5/site-packages/mne-python/mne/stats/permutations.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     max_abs = np.concatenate(parallel(my_max_stat(X, X2, p, dof_scaling)\n\u001b[0;32m--> 137\u001b[0;31m                                       for p in np.array_split(perms, n_jobs)))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0mH0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_abs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/karim/anaconda3/lib/python3.5/site-packages/mne-python/mne/stats/permutations.py\u001b[0m in \u001b[0;36m_max_stat\u001b[0;34m(X, X2, perms, dof_scaling)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m\"\"\"Aux function for permutation_t_test (for parallel comp).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mmus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mstds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmus\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdof_scaling\u001b[0m  \u001b[0;31m# std with splitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mmax_abs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmus\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstds\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# t-max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "st = study('Olfacto')\n",
    "path_data = path.join (st.path, 'database/TS_E_all_by_odor_th40_art400_30_250_5s_concatOK/')\n",
    "save_path = path.join(st.path, 'feature/ERP_Groups_Odors_250ms_rescale_filtered_stats/Significant/')\n",
    "\n",
    "test = False\n",
    "\n",
    "if test == True:\n",
    "    n_elec = {'SEMC' :2}\n",
    "    subjects = ['SEMC']\n",
    "else :\n",
    "    subjects = ['SEMC','PIRJ','LEFC','MICP','CHAF','VACJ'] \n",
    "    n_elec = {\n",
    "    'CHAF' : 107,\n",
    "    'VACJ' : 139, \n",
    "    'SEMC' : 107,\n",
    "    'PIRJ' : 106,\n",
    "    'LEFC' : 193,\n",
    "    'MICP' : 105,\n",
    "        }\n",
    "\n",
    "for su in subjects:\n",
    "    all_elec_p_val = np.array([])\n",
    "    all_elec_T =np.array([])\n",
    "    for elec in range(0, n_elec[su],1):\n",
    "        #Load files\n",
    "        badname = su+'_concat_odor_bad_bipo.npz'\n",
    "        goodname = su+'_concat_odor_good_bipo.npz'\n",
    "        data_bad = np.load(path.join(path_data, badname))\n",
    "        data_good = np.load(path.join(path_data, goodname))\n",
    "        data_bad, channel, label, data_good = data_bad['x'], data_bad['channel'], data_bad['label'], data_good['x']\n",
    "\n",
    "        # Select data for one elec + name :\n",
    "        data_elec_bad = data_bad[elec,:,:]\n",
    "        data_elec_good = data_good[elec,:,:]\n",
    "        ntrials = str(data_elec_bad.shape[1])+'/'+ str(data_elec_good.shape[1])\n",
    "        print ('Channel : ', channel[elec], 'Label : ', label[elec], 'N_trials :', ntrials, \n",
    "               'Bad shape : ', data_elec_bad.shape, 'Good shape : ', data_elec_good.shape)\n",
    "\n",
    "        #Filter data for one elec (all trials):\n",
    "        data_elec_bad = np.array(data_elec_bad, dtype='float64')\n",
    "        data_elec_good = np.array(data_elec_good, dtype='float64')\n",
    "        data_bad_to_filter = np.swapaxes(data_elec_bad, 0, 1)\n",
    "        data_good_to_filter = np.swapaxes(data_elec_good, 0, 1)\n",
    "        filtered_data_bad = filter_data(data_bad_to_filter, sfreq=512, l_freq=None, h_freq=low_pass_filter, method='fir', phase='zero-double')\n",
    "        filtered_data_good = filter_data(data_good_to_filter, sfreq=512, l_freq=None, h_freq=low_pass_filter, method='fir', phase='zero-double')\n",
    "        print ('Size of filtered data bad :', filtered_data_bad.shape, 'filtered data good : ', filtered_data_good.shape,)\n",
    "\n",
    "        #Normalize the non-averaged data (all trials)\n",
    "        times = np.arange(filtered_data_bad.shape[1])\n",
    "        print ('time points : ', times.shape)\n",
    "        norm_filtered_data_bad = rescale(filtered_data_bad, times=times, baseline=baseline, mode=norm_mode)\n",
    "        norm_filtered_data_good = rescale(filtered_data_good, times=times, baseline=baseline, mode=norm_mode)\n",
    "        print ('Size norm & filtered data 0 : ', norm_filtered_data_bad.shape, norm_filtered_data_good.shape,)\n",
    "\n",
    "        # =======================================  STATISTICS  =====================================\n",
    "        # Range of the data to compute\n",
    "        data_range = range(data_to_use[0], data_to_use[1])\n",
    "\n",
    "        # Get all three learning files (Filtered data)\n",
    "        data_bad = norm_filtered_data_bad[:, data_range]\n",
    "        data_good = norm_filtered_data_good[:, data_range,]\n",
    "        print ('-> Shape of the selected data for learn 0', data_bad.shape, 'learn 1', data_good.shape,)\n",
    "        \n",
    "        n_rep = 100\n",
    "        T_rep = np.array([])\n",
    "        p_val_rep = np.array([])\n",
    "        alpha = 0.05\n",
    "        for i in range(n_rep):\n",
    "            #reshape data to have the exact same nb of trials (mandatory for t-tests)\n",
    "            if data_bad.shape[0] > data_good.shape[0]:\n",
    "                data_bad = data_bad[np.random.randint(data_bad.shape[0], size=data_good.shape[0]), :] #reshape bad_data to fit good_data shape\n",
    "                print ('rand bad matrix', data_bad.shape)\n",
    "            if data_bad.shape[0] < data_good.shape[0]:\n",
    "                data_good = data_good[np.random.randint(data_good.shape[0], size=data_bad.shape[0]), :]\n",
    "                print ('rand good matrix', data_good.shape)\n",
    "            X = data_bad - data_good #the last dimension needs to be time\n",
    "            T0, p_values, H0 = permutation_t_test(X, n_permutations=1000, tail=0, n_jobs=1, verbose=None)\n",
    "            T_rep = np.vstack((T_rep,T0)) if np.size(T_rep) else T0\n",
    "            p_val_rep = np.vstack((p_val_rep,p_values)) if np.size(p_val_rep) else p_values\n",
    "            \n",
    "        idx_signif_nb = []\n",
    "        for s in range(n_rep):\n",
    "            if p_val_rep[s,:].min() < alpha:\n",
    "                idx_signif_nb.append(1)\n",
    "            else:\n",
    "                idx_signif_nb.append(0)\n",
    "        print (idx_signif_nb)\n",
    "        if sum(idx_signif_nb) >= n_rep - n_rep*alpha:\n",
    "            # save all pvalues\n",
    "            fname = (save_path +su +'pvalues_good_bad_' + channel [elec] +'_'+str(elec)+'_'+label[elec]+'.npy')\n",
    "            np.save(fname, p_val_rep)\n",
    "            \n",
    "            # plot and save pvalues\n",
    "            plot_name = (save_path +su +'pvalues_good_bad_' + channel [elec] +'_'+str(elec)+'_'+label[elec]+'.png')\n",
    "            times_plot = 1000 * np.arange((baseline[0] - baseline[1]), data_learn_to_plot.shape[0]-baseline[1] + baseline[0],) / sf\n",
    "            #Prepare the plot\n",
    "            fig = plt.figure(0, figsize=(12, 7))\n",
    "            ax = fig.add_subplot(111)\n",
    "            fig.subplots_adjust(top=0.85)\n",
    "            ax.set_xlabel('Times (ms)', fontsize=12)\n",
    "            ax.set_ylabel('pvalues Good-Bad', fontsize=12)\n",
    "            #Plot the data\n",
    "            BorderPlot(times_plot, p_val_rep.swapaxes(0,1), kind='sem', alpha=0.2, color=['r'], \n",
    "                       linewidth=2, ncol=1, title=su+'_pvalues_'+str(channel[elec])+'_'+str(label[elec])+' elec_num: '+str(elec)+'_nrep:'+str(n_rep),)\n",
    "            plt.gca()\n",
    "            lines = [0] #time vector is in ms\n",
    "            addPval(plt.gca(), p_values, p=0.05, x=times_plot, y=0.5, color='b', lw=3)\n",
    "            addLines(plt.gca(), vLines=lines, vColor=['firebrick'], vWidth=[2], hLines=[0], hColor=['#000000'], hWidth=[2])\n",
    "            plt.grid()\n",
    "            plt.savefig(plot_name, dpi=300, bbox_inches='tight')\n",
    "            plt.clf()\n",
    "            plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
