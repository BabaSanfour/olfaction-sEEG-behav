{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import join, exists\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from brainpipe.system import study\n",
    "from mne.stats import fdr_correction, bonferroni_correction\n",
    "from utils import rename_elecs, odors_su_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def su_gr_score_all(su,data):\n",
    "    score_ = []\n",
    "    for i,data_sel in enumerate(data):\n",
    "        score_.extend([i+1]*data_sel.shape[-1])\n",
    "    return score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Correlate TPSim through learning as a function of trial order\n",
    ">>>> SEMC and LEFC could be sliced differently (reducing the early group)\n",
    "BY ELECTRODE and Plot summary (for all included electrodes)\n",
    "\"\"\"\n",
    "freq, meth = 'theta', '2gr' #'1_alpha', '2_beta','3_gamma', high_gamma\n",
    "rois_sel = ['OFC_olf']\n",
    "st = study('Olfacto')\n",
    "###############################################################################\n",
    "path_tps = join(st.path, 'feature/TPSim_3groups_Enc/TPS_btw_thgh_time_v=1_elecs=all/')\n",
    "tps_form = join(path_tps, 'TPS_pears_learn_{}_btw_{}_{}.npz')\n",
    "df_name = join(path_tps, '{}_ols_btw_{}_{}_{}.csv') #su, conds0, conds1, freq\n",
    "###############################################################################\n",
    "    \n",
    "subjects = ['CHAF','LEFC','PIRJ','VACJ','SEMC','FERJ']\n",
    "subjects_c, elecs_c, labels_c = np.array([]), np.array([]), np.array([])\n",
    "tps_scores,channels_c, x_c, y_c, z_c = np.array([]),np.array([]), np.array([]), np.array([]), np.array([])\n",
    "tps_sem, T_vals_c, p_vals_c = np.array([]), np.array([]),np.array([])\n",
    "p_fdr_c, p_bf_c = np.array([]), np.array([])\n",
    "\n",
    "for su in subjects:\n",
    "    mat = np.load(tps_form.format(su,freq,meth),allow_pickle=True)\n",
    "\n",
    "    #load all elec info,rename and select electrodes id\n",
    "    labels, channels = mat['label'], mat['channel']\n",
    "    x, y, z = mat['xyz'][:,0], mat['xyz'][:,1], mat['xyz'][:,2]\n",
    "    labels_new = rename_elecs(labels,x,y,z)\n",
    "    idx_sel = [i for i,lab in enumerate(labels_new) if lab in rois_sel]\n",
    "\n",
    "    #selected data\n",
    "    labels, channels = labels_new[idx_sel], channels[idx_sel]\n",
    "    x, y, z = x[idx_sel], y[idx_sel], z[idx_sel]\n",
    "\n",
    "    #load all data by trials\n",
    "    tps_su, tps_sem_su = np.zeros((labels.shape[0],2)),np.zeros((labels.shape[0],2))\n",
    "    data = []\n",
    "    for n in range(len(mat.files)-3):\n",
    "        data.append(mat['tps_'+str(n)][idx_sel,:])\n",
    "    print('shape of tps data', [x.shape for x in data])\n",
    "    \n",
    "    #split data in 3 groups as a function of time and nb of trials\n",
    "    if meth in ['3gr','btw_trials']:\n",
    "        concat_ = np.concatenate(data,axis=-1)\n",
    "        ntrials = np.sum([x.shape[-1] for x in data])\n",
    "        nelecs = concat_.shape[0]\n",
    "        print('concat', concat_.shape)\n",
    "        if meth in ['3gr','btw_trials']:\n",
    "            score_ = [0]*data[0].shape[-1]+[1]*data[1].shape[-1]+[2]*data[2].shape[-1]\n",
    "        if meth == 'all':\n",
    "            score_ = su_gr_score_all(su,data)\n",
    "    if meth == '2gr':\n",
    "        early, late = data[0],data[1]\n",
    "        tps_su[:,0] += np.mean(early,axis=1)\n",
    "        tps_sem_su[:,0] += stats.sem(early,axis=1)\n",
    "        tps_su[:,1] += np.mean(late,axis=1)\n",
    "        tps_sem_su[:,1] += stats.sem(late,axis=1)\n",
    "        nelecs = early.shape[0]\n",
    "        print(early.shape,late.shape)\n",
    "\n",
    "    #compute stats Ttests-unpaired\n",
    "    if meth in ['3gr','btw_trials']:\n",
    "        T, unc_p = [], []\n",
    "        for elec in range(nelecs):\n",
    "            #Tval,pval = stats.kendalltau(all_tps[elec],all_scores)\n",
    "            Y, X = np.array(concat_[elec]), sm.add_constant(np.array(score_))\n",
    "            model_ols = sm.OLS(Y,X).fit()\n",
    "            Tval, pval = np.round(model_ols.tvalues[1],3),model_ols.pvalues[1]\n",
    "            T.append(Tval), unc_p.append(pval)\n",
    "    if meth == '2gr':\n",
    "        T, unc_p = stats.ttest_ind(early,late,axis=1,equal_var=False)\n",
    "    _, p_fdr = fdr_correction(unc_p)\n",
    "    _, p_bf = bonferroni_correction(unc_p)\n",
    "\n",
    "    #fill all df data\n",
    "    if early.shape[0] > 0:\n",
    "        subjects_c = np.hstack((subjects_c,np.array([su]*nelecs))) if np.size(subjects_c) else np.array([su]*nelecs)\n",
    "        elecs_c = np.hstack((elecs_c,np.arange(nelecs))) if np.size(elecs_c) else np.arange(nelecs)\n",
    "        labels_c = np.hstack((labels_c,labels)) if np.size(labels_c) else labels\n",
    "        channels_c = np.hstack((channels_c,channels)) if np.size(channels_c) else channels\n",
    "        tps_scores = np.concatenate((tps_scores,tps_su)) if np.size(tps_scores) else tps_su\n",
    "        tps_sem = np.concatenate((tps_sem,tps_sem_su)) if np.size(tps_sem) else tps_sem_su\n",
    "        x_c = np.hstack((x_c,x)) if np.size(x_c) else x\n",
    "        y_c = np.hstack((y_c,y)) if np.size(y_c) else y\n",
    "        z_c = np.hstack((z_c,z)) if np.size(z_c) else z\n",
    "        T_vals_c = np.hstack((T_vals_c,T)) if np.size(T_vals_c) else T\n",
    "        p_vals_c = np.hstack((p_vals_c,unc_p)) if np.size(p_vals_c) else unc_p\n",
    "        p_fdr_c = np.hstack((p_fdr_c,p_fdr)) if np.size(p_fdr_c) else p_fdr\n",
    "        p_bf_c = np.hstack((p_bf_c,p_bf)) if np.size(p_bf_c) else p_bf\n",
    "\n",
    "pvals_all_fdr = fdr_correction(p_vals_c)[1][:,np.newaxis]\n",
    "data_all = np.concatenate((subjects_c[:,np.newaxis],labels_c[:,np.newaxis],\n",
    "            channels_c[:,np.newaxis],x_c[:,np.newaxis],y_c[:,np.newaxis],\n",
    "            z_c[:,np.newaxis],elecs_c[:,np.newaxis],tps_scores,tps_sem,T_vals_c[:,np.newaxis],\n",
    "            p_vals_c[:,np.newaxis],p_fdr_c[:,np.newaxis],p_bf_c[:,np.newaxis],pvals_all_fdr),\n",
    "            axis=1)\n",
    "df = pd.DataFrame(data_all, columns=['subjects','labels','channels','x','y','z',\n",
    "                        'elecs_num', 'tps_early', 'tps_late','tps_early_sem','tps_late_sem',\n",
    "                                     'Tvals', 'unc_p','fdr_p', 'bonf_p','fdr_all'])\n",
    "print(df.shape)\n",
    "df.to_csv(df_name.format('All_subjects',freq,'learn'+meth,rois_sel[0]),index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "st = study('Olfacto')\n",
    "olf_regions = ['Amg','pPirT','OFC_olf','Ins_olf']\n",
    "freq, meth = 'theta', '2gr'\n",
    "thrs = [0.05]\n",
    "corrections = ['fdr_all']#['fdr_p','bonf_p']\n",
    "\n",
    "##################################################################################\n",
    "path_pow = join(st.path, 'feature/TPSim_3groups_Enc/TPS_btw_thgh_time_v=1_elecs=all/')\n",
    "df_name = join(path_pow, '{}_ols_btw_{}_learn'+meth+'_aHC.csv') #su, conds0, conds1, freq\n",
    "df_stat_save = join(path_pow, 'Bilan_{}_OLS_{}_{}_{}_{}.csv')\n",
    "##################################################################################\n",
    "\n",
    "df = pd.read_csv(df_name.format('All_subjects',freq))\n",
    "print('Initial df shape', df.shape)\n",
    "\n",
    "for th, corr in product(thrs,corrections):\n",
    "    df_sel = df.loc[df[corr]<th]\n",
    "    print(df_sel)\n",
    "    #df_sel.to_csv(df_stat_save.format('All_subjects','theta',meth,roi,corr+str(th)))\n",
    "    df_sel['sign'] = ['separation' if t > 0 else 'completion' for t in df_sel['Tvals']]\n",
    "    print('stats at p < ',th, 'correction : ',corr, df_sel.shape)\n",
    "    print(Counter(df['labels'].loc[df[corr]<th]))\n",
    "\n",
    "    rois = np.unique(df_sel['labels'])\n",
    "    for roi in rois:\n",
    "        if roi in ['aHC','OFC_olf','IFG']:\n",
    "            df_roi = df_sel.loc[df_sel['labels']==roi]\n",
    "            #print(df_roi)\n",
    "            df_inc = df_roi.loc[df_roi['sign']=='completion'].groupby(['subjects']).count()\n",
    "            df_dec = df_roi.loc[df_roi['sign']=='separation'].groupby(['subjects']).count()\n",
    "\n",
    "            if (df_inc.shape[0] >= 1) or (df_inc.shape[0] >=2 and roi in olf_regions):\n",
    "                print(roi, 'NB of subjects with completion',df_inc.shape[0],' subjects')\n",
    "                df_plot = df_roi.loc[df_roi['sign']=='completion']\n",
    "                print('#electrodes in total >>> ',df_plot.shape[0])\n",
    "                df_plot.to_csv(df_stat_save.format('All_subjects','theta',meth,roi,corr+str(th)))\n",
    "                print(df_plot)\n",
    "                \n",
    "            if (df_dec.shape[0] >= 1) or (df_dec.shape[0] >=2 and roi in olf_regions):\n",
    "                print(roi, 'NB of subjects with separation',df_dec.shape[0],' subjects')\n",
    "                df_plot = df_roi.loc[df_roi['sign']=='separation']\n",
    "                print('#electrodes in total >>> ',df_plot.shape[0])\n",
    "                df_plot.to_csv(df_stat_save.format('All_subjects','theta',meth,roi,corr+str(th)))\n",
    "                print(df_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Early Late effects\n",
    "Individual results bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainpipe.system import study\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "st = study('Olfacto')\n",
    "PATH = join(st.path, 'feature/TPSim_3groups_{}/TPS_btw_thgh_time_v=1_elecs=all/')\n",
    "file_name = join(PATH, 'All_subjects_ols_btw_theta_learn2gr_{}.csv')\n",
    "SAVE_PATH = join(PATH, 'Ind_elecs_{}_{}_avg_subj_2gr_not_sig.png')\n",
    "meth, exp, freq, rois = 'btw', 'Enc', 'theta', ['aHC','OFC_olf']\n",
    "features = ['tps_early','tps_late']\n",
    "\n",
    "for roi in rois:\n",
    "    df = pd.read_csv(file_name.format(exp,roi))\n",
    "    df_m = df.groupby(['subjects']).agg(['mean','sem'])\n",
    "    nelecs = df_m.shape[0]\n",
    "\n",
    "    fig, axs = plt.subplots(2,4, figsize=(15, 6), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for i in range(nelecs):\n",
    "        df_sel = df_m.iloc[i,:].unstack().T\n",
    "        #print(df_sel)\n",
    "\n",
    "        su =  df_m.index[i]\n",
    "        data = df_sel[features].iloc[0,:].values\n",
    "#         yerr = df_sd.iloc[i,:][features].values\n",
    "        yerr = df_sel[features].iloc[1,:].values\n",
    "        #print(data,yerr)\n",
    "\n",
    "        xticks, w = np.arange((len(features))), 0.8\n",
    "        axs[i].set_xticks(xticks,features)\n",
    "        axs[i].bar(xticks,data,yerr=yerr,color='blue')\n",
    "        axs[i].set_title('{} in {}'.format(su,roi))\n",
    "    \n",
    "    plt.setp(axs, ylim=(0,1)) if roi == 'aHC' else plt.setp(axs, ylim=(0,1.1))\n",
    "    plt.savefig(SAVE_PATH.format(exp,'tps',roi))\n",
    "    plt.savefig(SAVE_PATH.format(exp,'tps',roi).replace('.png','.pdf'))\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def tpsim_by_cond(pow1, stat='pearson',average=True):\n",
    "    \"\"\"\n",
    "    Compute tpsim within one condition for all combinations\n",
    "    Parameters\n",
    "    ----------\n",
    "    pow1 : array \n",
    "        Must be of shape (npts x ntrials)\n",
    "    stat : string\n",
    "        The stat correlation method to use. 'pearson' or 'spearman'\n",
    "    Returns\n",
    "    -------\n",
    "    tpsim : array\n",
    "    \"\"\"\n",
    "    \n",
    "    corr = pearsonr if stat == 'pearson' else spearmanr\n",
    "    print(pow1.shape)\n",
    "    sim_trials = np.array([])\n",
    "    for t0, t1 in combinations(np.arange(pow1.shape[-1]),2):\n",
    "        R, _ = corr(pow1[:,t0],pow1[:,t1])\n",
    "        sim_trials = np.vstack((sim_trials,1-R)) if np.size(sim_trials) else np.array([1-R])\n",
    "    print('sim_trials shape', sim_trials.shape)\n",
    "    if np.size(sim_trials) == 1:\n",
    "        sim_trials = np.array([[sim_trials]])\n",
    "    else:\n",
    "        sim_trials = sim_trials.swapaxes(0,1)\n",
    "    if average == True:\n",
    "        sim_trials = np.mean(sim_trials)\n",
    "    return sim_trials\n",
    "\n",
    "def early_late_pow(pow_data):\n",
    "    thr = int(pow_data.shape[-1]/2)\n",
    "    if thr == 0:\n",
    "        early = np.array([])\n",
    "        late = np.array([])\n",
    "    else:\n",
    "        early = pow_data[...,:thr]\n",
    "        late = pow_data[...,thr:]\n",
    "    return early, late\n",
    "\n",
    "def tpsim_btw_2odors(pow_o1,pow_o2,stat='pearson',average=True):\n",
    "    \"\"\"\n",
    "    Compute tpsim between 2 odors \n",
    "    Parameters\n",
    "    ----------\n",
    "    pow_o1, pow_o2 : array (npts x ntrials)\n",
    "    stat : string\n",
    "            The stat correlation method to use. 'pearson' or 'spearman'\n",
    "    Returns\n",
    "    -------\n",
    "    tpsim : array (Average TPSim between 2 ODORS)\n",
    "    \"\"\"\n",
    "    \n",
    "    corr = stats.pearsonr if stat == 'pearson' else stats.spearmanr\n",
    "    sim_trials = np.array([])\n",
    "    for t0, t1 in product(range(pow_o1.shape[-1]),range(pow_o2.shape[-1])):\n",
    "        R, _ = corr(pow_o1[:,t0],pow_o2[:,t1])\n",
    "        sim_trials = np.vstack((sim_trials,1-R)) if np.size(sim_trials) else np.array([1-R])\n",
    "    if average == True:\n",
    "        sim_trials = np.mean(sim_trials)\n",
    "    return sim_trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement, combinations\n",
    "\"\"\"\n",
    "Compute TPSim BTW odors By ODOR EARLY and LATE ALLTOGETHER\n",
    ">>> in order to compute dissimilarity matrices EARLY and LATE\n",
    "\"\"\"\n",
    "###############################################################################\n",
    "st = study('Olfacto')\n",
    "path_pow = join(st.path, 'feature/TPSim_power_data/Power_all_elecs_E_R_by_odor_all_elecsFT/')\n",
    "pow_path = join(path_pow, '{}_odor_{}_bipo_all_{}_6freqs.npz')\n",
    "path2save = join(st.path, 'feature/TPSim_3groups_{}/similarity_matrix_thgh_time_btw_v=1_elecs=all/')\n",
    "tps_save = join(path2save, 'TPS_{}_{}_{}_EL_btw_odors_mean.npz')\n",
    "###############################################################################\n",
    "exp, stat = 'Enc', 'pearson' #Ret, Enc\n",
    "shortcut = ['E','R'] if exp == 'Enc_Ret' else [exp[0]]\n",
    "if not exists(path2save.format(exp)):\n",
    "    makedirs(path2save.format(exp))\n",
    "###############################################################################\n",
    "freqs, f = ['theta'], 1\n",
    "odors_su = {'CHAF': [1,2,4,5,3,8,7,9], #low than high\n",
    "            'LEFC': [15,2,1,16,14,3,4,17],\n",
    "            'PIRJ': [1,9,5,4,6,7,18], #missing odor 15\n",
    "            'VACJ': [11,14,12,10,15,17,16,13],\n",
    "            'SEMC': [7,10,11,12,13,5,8,9],\n",
    "            'FERJ': [7,2,16,17,12,1,5,13]}\n",
    "    \n",
    "for su,freq in product(odors_su,freqs):\n",
    "    list_od = []\n",
    "    for od in odors_su[su]:\n",
    "        pow_ = np.load(pow_path.format(su,str(od),exp[0],freq))['xpow']\n",
    "        if pow_.shape[-1] >= 2:\n",
    "            list_od.append(od)\n",
    "    odors_EL = [str(o)+'_E' for o in list_od] + [str(o)+'_L' for o in list_od]\n",
    "    ncomb = len([c for c in combinations(odors_EL,2)]) #different odors only\n",
    "    od_sel = odors_EL[0].split('_')[0]\n",
    "    mat_ = np.load(pow_path.format(su,od_sel,exp[0],freq),allow_pickle=True)\n",
    "    chans, labels, xyz = mat_['channels'], mat_['labels'], mat_['xyz']\n",
    "    nelecs = mat_['xpow'][f,...].shape[0]\n",
    "    print(su,freq,list_od,'nb comb',ncomb,'nelecs',nelecs)\n",
    "    \n",
    "    tps = np.zeros((nelecs,ncomb))\n",
    "    list_comb = np.array([])\n",
    "    for i,comb in enumerate(combinations(odors_EL,2)):\n",
    "        print('o1',comb[0],'o2',comb[1])\n",
    "        mat_o1 = np.load(pow_path.format(su,comb[0].split('_')[0],exp[0],freq),allow_pickle=True)\n",
    "        pow_o1 = mat_o1['xpow'][f,:,17:47,:] #nelecs,npts,ntrials\n",
    "        mat_o2 = np.load(pow_path.format(su,comb[1].split('_')[0],exp[0],freq),allow_pickle=True)\n",
    "        pow_o2 = mat_o2['xpow'][f,:,17:47,:] #nelecs,npts,ntrials\n",
    "\n",
    "        o1_E, o1_L = early_late_pow(pow_o1)\n",
    "        o2_E, o2_L = early_late_pow(pow_o2)\n",
    "        print(o1_E.shape,o1_L.shape,o2_E.shape,o2_L.shape,)\n",
    "        \n",
    "        list_comb = np.vstack((list_comb,comb)) if np.size(list_comb) else comb\n",
    "        \n",
    "        for elec in range(nelecs):\n",
    "            o1_sel = o1_E if comb[0].split('_')[1] == 'E' else o1_L\n",
    "            o2_sel = o2_E if comb[1].split('_')[1] == 'E' else o2_L\n",
    "            tps[elec,i] += tpsim_btw_2odors(o1_sel[elec],o2_sel[elec],average=True)\n",
    "            \n",
    "    np.savez(tps_save.format(exp,stat,su,freq),tps_EL=tps,comb=list_comb,channels=chans,\n",
    "                labels=labels, xyz=xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\"\"\"\n",
    "Compute TPSim BTW odors By ODOR FOR EARLY or LATE separately\n",
    ">>> in order to compute dissimilarity matrices EARLY and LATE\n",
    "\"\"\"\n",
    "###############################################################################\n",
    "st = study('Olfacto')\n",
    "path_pow = join(st.path, 'feature/TPSim_power_data/Power_all_elecs_E_R_by_odor_all_elecsFT/')\n",
    "pow_path = join(path_pow, '{}_odor_{}_bipo_all_{}_6freqs.npz')\n",
    "path2save = join(st.path, 'feature/TPSim_3groups_{}/similarity_matrix_thgh_time_btw_v=1_elecs=all/')\n",
    "tps_save = join(path2save, 'TPS_{}_{}_{}_btw_odors_mean.npz')\n",
    "###############################################################################\n",
    "exp, stat = 'Enc', 'pearson' #Ret, Enc\n",
    "shortcut = ['E','R'] if exp == 'Enc_Ret' else [exp[0]]\n",
    "if not exists(path2save.format(exp)):\n",
    "    makedirs(path2save.format(exp))\n",
    "###############################################################################\n",
    "freqs, f = ['theta'], 1\n",
    "odors_su = {'CHAF': [1,2,4,5,3,8,7,9], #low than high\n",
    "            'LEFC': [15,2,1,16,14,3,4,17],\n",
    "            'PIRJ': [1,9,5,4,6,7,18], #missing odor 15\n",
    "            'VACJ': [11,14,12,10,15,17,16,13],\n",
    "            'SEMC': [7,10,11,12,13,5,8,9],\n",
    "            'FERJ': [7,2,16,17,12,1,5,13]}\n",
    "    \n",
    "for su,freq in product(odors_su,freqs):\n",
    "    list_od = []\n",
    "    for od in odors_su[su]:\n",
    "        pow_ = np.load(pow_path.format(su,str(od),exp[0],freq))['xpow']\n",
    "        if pow_.shape[-1] >= 2:\n",
    "            list_od.append(od)\n",
    "    ncomb = len([c for c in combinations_with_replacement(list_od,2)])\n",
    "    od_sel = list_od[0]\n",
    "    mat_ = np.load(pow_path.format(su,str(od_sel),exp[0],freq),allow_pickle=True)\n",
    "    chans, labels, xyz = mat_['channels'], mat_['labels'], mat_['xyz']\n",
    "    nelecs = mat_['xpow'][f,...].shape[0]\n",
    "    print(su,freq,list_od,'nb comb',ncomb,'nelecs',nelecs)\n",
    "    tpsE, tpsL = np.zeros((nelecs,ncomb)), np.zeros((nelecs,ncomb))\n",
    "    \n",
    "    list_comb = np.array([])\n",
    "    for i,comb in enumerate(combinations_with_replacement(list_od,2)):\n",
    "        print('o1',comb[0],'o2',comb[1])\n",
    "        mat_o1 = np.load(pow_path.format(su,comb[0],exp[0],freq),allow_pickle=True)\n",
    "        pow_o1 = mat_o1['xpow'][f,:,17:47,:] #nelecs,npts,ntrials\n",
    "        mat_o2 = np.load(pow_path.format(su,comb[1],exp[0],freq),allow_pickle=True)\n",
    "        pow_o2 = mat_o2['xpow'][f,:,17:47,:] #nelecs,npts,ntrials\n",
    "        o1_E, o1_L = early_late_pow(pow_o1)\n",
    "        o2_E, o2_L = early_late_pow(pow_o2)\n",
    "        print(o1_E.shape,o1_L.shape,o2_E.shape,o2_L.shape,)\n",
    "        \n",
    "        list_comb = np.vstack((list_comb,comb)) if np.size(list_comb) else comb\n",
    "        \n",
    "        for elec in range(nelecs):\n",
    "            if comb[0] != comb[1]: #different odors\n",
    "                tpsE[elec,i] += tpsim_btw_2odors(o1_E[elec],o2_E[elec],average=True)\n",
    "                tpsL[elec,i] +=tpsim_btw_2odors(o1_L[elec],o2_L[elec],average=True)\n",
    "            else: #if same odor o1 = o2\n",
    "                if o1_E.shape[-1] > 1: #compute similiarities across repetitions of the same odor\n",
    "                    tpsE[elec,i] += tpsim_by_cond(o1_E[elec],average=True)\n",
    "                if o1_L.shape[-1] > 1:\n",
    "                    tpsL[elec,i] += tpsim_by_cond(o1_L[elec],average=True)\n",
    "            \n",
    "    np.savez(tps_save.format(exp,stat,su,freq),tpsE=tpsE, tpsL=tpsL,comb=list_comb,channels=chans,\n",
    "                labels=labels, xyz=xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distance matrix for Early and Late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib import cm\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "from brainpipe.system import study\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import random\n",
    "from utils import  odor_groups_3wgth\n",
    "\"\"\"\n",
    "Plot TPSim matrices (RDM) for all odors \n",
    "\"\"\"\n",
    "###############################################################################\n",
    "st = study('Olfacto')\n",
    "path_df = join(st.path, 'feature/TPSim_3groups_Enc/TPS_btw_thgh_time_v=1_elecs=all/')\n",
    "df_name = join(path_df, 'Bilan_All_subjects_OLS_theta_2gr_{}_fdr_all0.05.csv')\n",
    "path_npz = join(st.path,'feature/TPSim_3groups_Enc/similarity_matrix_thgh_time_btw_v=1_elecs=all')\n",
    "npz_name = join(path_npz, 'TPS_pearson_{}_theta_EL_btw_odors_mean.npz')\n",
    "savename = join(path_npz, 'Plot_distance_{}_{}_{}_EL.png')\n",
    "###############################################################################\n",
    "rois = ['aHC','OFC_olf']\n",
    "conds = ['tps_EL']\n",
    "\n",
    "for roi in rois:\n",
    "    df = pd.read_csv(df_name.format(roi))\n",
    "    subjects = np.unique(df['subjects'])\n",
    "    for su in subjects:\n",
    "        df_su = df.loc[df['subjects']==su]\n",
    "        chans_df = df_su['channels'].values\n",
    "        \n",
    "        mat = np.load(npz_name.format(su),allow_pickle=True)\n",
    "        print(su,'shape', mat[conds[0]].shape)\n",
    "        idx_elecs = [i for i,chan in enumerate(mat['channels']) if chan in chans_df]\n",
    "        combs = mat['comb']\n",
    "        print(mat['labels'][idx_elecs], mat['channels'][idx_elecs])\n",
    "        n_od = len(np.unique(combs))\n",
    "        print('n_od',n_od)\n",
    "        idx = [combs[0,0]]+[o for o in combs[:n_od-1,1]]\n",
    "        #idx = [o for o in combs[:n_od,1]]\n",
    "        \n",
    "        tps = mat[conds[0]][idx_elecs] if len(idx_elecs) == 1 else \\\n",
    "                                    np.mean(mat[conds[0]][idx_elecs],axis=0)\n",
    "        tri = np.zeros((n_od, n_od))\n",
    "        #tri[np.triu_indices(n_od)] = tps #diagonal of 0\n",
    "        #tri[np.tril_indices(n_od)] = tri.T[np.tril_indices(n_od)]\n",
    "        \n",
    "        tri[np.triu_indices(n_od,1)] = tps #diagonal of 0\n",
    "        tri[np.tril_indices(n_od, -1)] = tri.T[np.tril_indices(n_od, -1)]\n",
    "        \n",
    "        model = MDS(n_components=2, dissimilarity='precomputed') #random_state=1\n",
    "        out = model.fit_transform(tri)\n",
    "\n",
    "        fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(8,3))\n",
    "        colors = ['aqua']*int(n_od/2)+['darkorange']*int(n_od/2)\n",
    "        markers = 'o'\n",
    "        x_centroid_E = sum(out[:int(n_od/2),0])/int(n_od/2) \n",
    "        y_centroid_E = sum(out[:int(n_od/2),0])/int(n_od/2)\n",
    "        x_centroid_L = sum(out[int(n_od/2):,0])/int(n_od/2)\n",
    "        y_centroid_L = sum(out[int(n_od/2):,0])/int(n_od/2)\n",
    "        \n",
    "        for i, txt in enumerate(idx):\n",
    "            ax1.scatter(out[i,0], out[i,1], c=colors[i], marker=markers,s=6)\n",
    "            ax1.annotate(txt, (out[i,0], out[i,1]))\n",
    "\n",
    "        for i, txt in enumerate(idx):\n",
    "            ax2.scatter(out[i,0], out[i,1], c=colors[i], marker=markers,s=6)\n",
    "        ax2.scatter(x_centroid_E, y_centroid_E, s=6,c='aqua', marker='*')\n",
    "        ax2.scatter(x_centroid_L, y_centroid_L, s=6,c='darkorange', marker='*')\n",
    "        \n",
    "        #subplot imshow matrix and mask upper triangle\n",
    "        mask =  np.tri(tri.shape[0], k=0) #mask upper triangle\n",
    "        for row,col in product(range(tri.shape[0]), range(tri.shape[1])):\n",
    "            if row < (tri.shape[0]/2):\n",
    "                if col > ((tri.shape[0]-1)/2):\n",
    "                    mask[row,col] = 1 #mask early vs late distances\n",
    "        \n",
    "        A = np.ma.array(tri, mask=mask) # mask out the lower triangle\n",
    "        cmap = cm.get_cmap('viridis', 30)\n",
    "        cmap.set_bad('w') # default value is 'k'\n",
    "        cax = ax3.imshow(A,vmin=0,vmax=1.8,interpolation=\"nearest\", cmap=cmap,aspect='auto')\n",
    "        ax3.set_xticks(np.arange(n_od))\n",
    "        ax3.set_yticks(np.arange(n_od))\n",
    "        ax3.set_xticklabels(idx,fontsize=11,rotation=60)\n",
    "        ax3.set_yticklabels(idx,fontsize=11,rotation=20)\n",
    "\n",
    "        asp = np.abs(np.diff(ax1.get_xlim())[0] / np.diff(ax1.get_ylim())[0])\n",
    "        ax1.set_aspect(asp)\n",
    "        ax2.set_aspect(asp)\n",
    "        \n",
    "        plt.colorbar(cax)\n",
    "        plt.subplots_adjust(top=0.92)\n",
    "        title = 'Distance btw odors in {} {} {}'.format(su,'early_late',roi)\n",
    "        fig.suptitle(title, fontsize=12)\n",
    "\n",
    "        plt.savefig(savename.format(su,'early_late',roi))\n",
    "        plt.savefig(savename.format(su,'early_late',roi).replace('.png','.pdf'))\n",
    "        plt.clf()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask =  np.tri(tri.shape[0], k=0)\n",
    "print(tri.shape)\n",
    "print(mask)\n",
    "\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib import cm\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "from brainpipe.system import study\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import random\n",
    "from utils import  odor_groups_3wgth\n",
    "\"\"\"\n",
    "Plot TPSim matrices (RDM) for all odors \n",
    "\"\"\"\n",
    "###############################################################################\n",
    "st = study('Olfacto')\n",
    "path_df = join(st.path, 'feature/TPSim_3groups_Enc/TPS_btw_thgh_time_v=1_elecs=all/')\n",
    "df_name = join(path_df, 'Bilan_All_subjects_OLS_theta_2gr_{}_fdr_all0.05.csv')\n",
    "path_npz = join(st.path,'feature/TPSim_3groups_Enc/similarity_matrix_thgh_time_btw_v=1_elecs=all')\n",
    "npz_name = join(path_npz, 'TPS_pearson_{}_theta_btw_odors_mean.npz')\n",
    "savename = join(path_npz, 'Plot_distance_{}_{}_{}_2D.png')\n",
    "###############################################################################\n",
    "rois = ['aHC','OFC_olf']\n",
    "conds = ['tpsE', 'tpsL']\n",
    "\n",
    "for roi in rois:\n",
    "    df = pd.read_csv(df_name.format(roi))\n",
    "    subjects = np.unique(df['subjects'])\n",
    "    for su in subjects:\n",
    "        df_su = df.loc[df['subjects']==su]\n",
    "        idx_elecs = df_su['elecs_num']\n",
    "        \n",
    "        mat = np.load(npz_name.format(su),allow_pickle=True)\n",
    "        print(su,'shape', mat['tpsE'].shape)\n",
    "        combs = mat['comb']\n",
    "        n_od = len(np.unique(combs))\n",
    "        print('n_od',n_od)\n",
    "        idx = list([o for o in combs[:n_od,1]])\n",
    "        \n",
    "        tpsE = mat[conds[0]][idx_elecs] if len(idx_elecs) == 1 else np.mean(mat[conds[0]][idx_elecs],axis=0)\n",
    "        triE = np.zeros((n_od, n_od))\n",
    "        triE[np.triu_indices(n_od)] = tpsE\n",
    "        triE[np.tril_indices(n_od)] = triE.T[np.tril_indices(n_od)]\n",
    "        \n",
    "        tpsL = mat[conds[1]][idx_elecs] if len(idx_elecs) == 1 else np.mean(mat[conds[1]][idx_elecs],axis=0)\n",
    "        triL = np.zeros((n_od, n_od))\n",
    "        triL[np.triu_indices(n_od)] = tpsL\n",
    "        triL[np.tril_indices(n_od)] = triL.T[np.tril_indices(n_od)]\n",
    "\n",
    "        model = MDS(n_components=2, dissimilarity='precomputed', random_state=1)\n",
    "        out0 = model.fit_transform(triE)\n",
    "        out1 = model.fit_transform(triL)\n",
    "        out_all = np.concatenate((out0,out1),axis=0)\n",
    "        idx_all = [str(i)+'_E' for i in idx]+[str(i)+'_L' for i in idx]\n",
    "\n",
    "        fig, (ax1) = plt.subplots(1,1,figsize=(4,3))\n",
    "        colors = ['purple']*n_od+['orange']*n_od\n",
    "        markers = 'o'\n",
    "\n",
    "        for i, txt in enumerate(idx_all):\n",
    "            print(i,txt)\n",
    "            ax1.scatter(out_all[i,0], out_all[i,1], c=colors[i], marker=markers)\n",
    "            ax1.annotate('O'+str(txt), (out_all[i,0], out_all[i,1]))\n",
    "        ax1.set_xlabel('component 1')\n",
    "        ax1.set_ylabel('component 2')\n",
    "        #ax1.axis('equal')\n",
    "        \n",
    "        plt.subplots_adjust(top=0.92)\n",
    "        title = 'Distance btw odors in {} {} {})'.format(su,cond,roi)\n",
    "        fig.suptitle(title, fontsize=12)\n",
    "\n",
    "        plt.savefig(savename.format(su,cond,roi))\n",
    "        plt.savefig(savename.format(su,cond,roi).replace('.png','.pdf'))\n",
    "        plt.clf()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select electrodes from memory effect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_elecs = join(st.path, 'feature/TPSim_3groups_Enc/LinReg_stats_theta_v=1_elecs=all/')\n",
    "path_df_learn = join(st.path, 'feature/TPSim_3groups_Enc/TPS_btw_thgh_time_v=1_elecs=all/')\n",
    "meth = '2gr'\n",
    "files = st.search('Bilan_All_subjects',folder = path_elecs)\n",
    "\n",
    "for fi in files:\n",
    "    df = pd.read_csv(path_elecs+fi)\n",
    "    roi_st = fi.split('_')[-4]+'_'+fi.split('_')[-3]+'_'+fi.split('_')[-2][:-4]\n",
    "    subjects = np.unique(df['subjects'].values)\n",
    "    dict_elecs = {}\n",
    "    for su in subjects:\n",
    "        df_sel = df.loc[df['subjects']==su]\n",
    "        dict_elecs[su] = list(df_sel['channels'].values)\n",
    "    \n",
    "    df2 = pd.read_csv(path_df_learn+'All_subjects_ols_btw_theta_learn'+meth+'.csv')\n",
    "    for i,su in enumerate(dict_elecs):\n",
    "        if i == 0:\n",
    "            df_elecs = df2.loc[(df2['subjects']==su) & (df2.channels.isin(dict_elecs[su]))]\n",
    "        else:\n",
    "            df_el = df2.loc[(df2['subjects']==su) & (df2.channels.isin(dict_elecs[su]))]\n",
    "            df_elecs = df_elecs.append(df_el)\n",
    "    df_elecs.to_csv(path_df_learn+'Bilan_elecs_mem_{}_{}.csv'.format(meth,roi_st),index=False)\n",
    "    print(df_elecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Early Late effect all mem groups together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem, ttest_rel, ttest_ind\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\"\"\"\n",
    "ANOVA à mesures répétées, Mixed ANOVA ou Ttest paired\n",
    "\"\"\"\n",
    "freq = 'theta' #'1_alpha', '2_beta','3_gamma', high_gamma\n",
    "st = study('Olfacto')\n",
    "###############################################################################\n",
    "path_tps = join(st.path, 'feature/TPSim_3groups_Enc/TPS_btw_thgh_time_v=1_elecs=all/')\n",
    "###############################################################################\n",
    "steps = ['early','late']\n",
    "\n",
    "files = st.search('Bilan_All_', folder=path_tps)\n",
    "pvals = []\n",
    "for fi in files:\n",
    "    df = pd.read_csv(path_tps+fi)\n",
    "    #df = df.groupby(['subjects']).mean()\n",
    "    n_rows = df.shape[0]\n",
    "    subj_ = np.concatenate([df.index.values]*len(steps))[:,np.newaxis]\n",
    "    tps = df[['tps_early','tps_late']].values.T.flatten()[:,np.newaxis]\n",
    "    steps_ = np.concatenate([[step]*n_rows for step in steps])[:,np.newaxis]\n",
    "    df_stat = pd.DataFrame(data=np.concatenate((subj_,tps,steps_),axis=1),\n",
    "                  columns=['subjs','tps','steps'])\n",
    "    df_stat['tps'] = df_stat['tps'].astype(float)\n",
    "    print(df_stat)\n",
    "    0/0\n",
    "    #md = smf.mixedlm(\"tps ~ steps\", df_stat, groups=df_stat[\"subjs\"])\n",
    "    #mdf = md.fit()\n",
    "    #print(mdf.summary())\n",
    "    Tval, pval = ttest_rel(df['tps_early'].values,df['tps_late'].values)\n",
    "    print(Tval, pval)\n",
    "    #anova = pg.rm_anova(dv='tps',within=['steps'],\n",
    "    #              subject='subjs', data=df_stat)\n",
    "    #pvals.append(anova['p-unc'][0])\n",
    "    #print(anova)\n",
    "    cols0 = [col for col in df_gr[conds_df] if col.endswith('0')]\n",
    "    cols1 = [col for col in df_gr[conds_df] if col.endswith('1')]\n",
    "    means0, means1 = df_gr[cols0].values.mean(), df_gr[cols1].values.mean()\n",
    "    sem0, sem1 = sem(np.ravel(df_gr[cols0].values)), sem(np.ravel(df_gr[cols1].values))\n",
    "    ax = plt.bar(np.arange(2),np.array([means0,means1]), yerr=np.array([sem0,sem1]))\n",
    "    print(fi)    \n",
    "    #ax = df[conds_df].mean().plot.bar(yerr=df[conds_df].sem())\n",
    "    plt.savefig(path_tps+fi.replace('.csv','.png').replace('Bilan','Fig'))\n",
    "    plt.savefig(path_tps+fi.replace('.csv','.pdf').replace('Bilan','Fig'))\n",
    "    plt.clf()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute sig elecs by mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\"\"\"\n",
    "Compute df with TPSim through learning as a function of subsequent memory effect\n",
    "\"\"\"\n",
    "freq, conds = 'theta', ['low','mid','high'] #'1_alpha', '2_beta','3_gamma', high_gamma\n",
    "rois_sel = ['aHC','MFG','ACC','IFG','Amg','pPirT','PHG','Ins_olf','OFC_olf','SFG']\n",
    "time_steps = ['early','late']\n",
    "st = study('Olfacto')\n",
    "###############################################################################\n",
    "path_tps = join(st.path, 'feature/TPSim_3groups_Enc/TPS_btw_thgh_time_v=1_elecs=all/')\n",
    "tps_form = join(path_tps, 'TPS_pears_learn_{}_btw_{}_{}_2gr.npz')\n",
    "df_name = join(path_tps, '{}_ols_btw_{}_{}.csv') #su, conds0, conds1, freq\n",
    "###############################################################################\n",
    "    \n",
    "subjects = ['CHAF','LEFC','PIRJ','VACJ','SEMC','FERJ']\n",
    "subjects_c, elecs_c, labels_c = np.array([]), np.array([]), np.array([])\n",
    "tps_scores, channels_c, x_c, y_c, z_c = np.array([]),np.array([]), np.array([]), np.array([]), np.array([])\n",
    "\n",
    "for su in subjects:\n",
    "    mat = np.load(tps_form.format(su,freq,conds[0]),allow_pickle=True)\n",
    "\n",
    "    #load all elec info,rename and select electrodes id\n",
    "    labels, channels = mat['label'], mat['channel']\n",
    "    x, y, z = mat['xyz'][:,0], mat['xyz'][:,1], mat['xyz'][:,2]\n",
    "    labels_new = rename_elecs(labels,x,y,z)\n",
    "    idx_sel = [i for i,lab in enumerate(labels_new) if lab in rois_sel]\n",
    "    nelecs = len(idx_sel)\n",
    "\n",
    "    #selected data\n",
    "    labels, channels = labels_new[idx_sel], channels[idx_sel]\n",
    "    x, y, z = x[idx_sel], y[idx_sel], z[idx_sel]\n",
    "\n",
    "    #load all data by trials\n",
    "    tps_su = np.zeros((labels.shape[0],6))\n",
    "    i = 0\n",
    "    for cond in conds:\n",
    "        data = np.load(tps_form.format(su,freq,cond))\n",
    "        for n in range(len(time_steps)):\n",
    "            tps_su[:,i] += np.mean(data['tps_'+str(n)][idx_sel,:],axis=1)\n",
    "            i += 1\n",
    "    #fill all df data\n",
    "    subjects_c = np.hstack((subjects_c,np.array([su]*nelecs))) if np.size(subjects_c) else np.array([su]*nelecs)\n",
    "    elecs_c = np.hstack((elecs_c,np.arange(nelecs))) if np.size(elecs_c) else np.arange(nelecs)\n",
    "    labels_c = np.hstack((labels_c,labels)) if np.size(labels_c) else labels\n",
    "    channels_c = np.hstack((channels_c,channels)) if np.size(channels_c) else channels\n",
    "    tps_scores = np.concatenate((tps_scores,tps_su)) if np.size(tps_scores) else tps_su\n",
    "    x_c = np.hstack((x_c,x)) if np.size(x_c) else x\n",
    "    y_c = np.hstack((y_c,y)) if np.size(y_c) else y\n",
    "    z_c = np.hstack((z_c,z)) if np.size(z_c) else z\n",
    "\n",
    "data = np.concatenate((subjects_c[:,np.newaxis],labels_c[:,np.newaxis],\n",
    "            channels_c[:,np.newaxis],x_c[:,np.newaxis],y_c[:,np.newaxis],\n",
    "            z_c[:,np.newaxis],elecs_c[:,np.newaxis],tps_scores),axis=1)\n",
    "df = pd.DataFrame(data, columns=['subjects','labels','channels','x','y','z',\n",
    "                'elecs_num', 'low_0','low_1', 'mid_0','mid_1',\n",
    "                'high_0','high_1'])\n",
    "print(df.shape)\n",
    "df.to_csv(df_name.format('All_subjects',freq,'learn_conds2gr'),index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\"\"\"\n",
    "ANOVA à mesures répétées pour les électrodes sig en mem LinReg\n",
    "\"\"\"\n",
    "freq, conds = 'theta', ['low','mid','high'] #'1_alpha', '2_beta','3_gamma', high_gamma\n",
    "rois_sel = ['aHC','MFG','ACC','IFG','Amg','pPirT','PHG','Ins_olf','OFC_olf','SFG']\n",
    "st = study('Olfacto')\n",
    "###############################################################################\n",
    "path_tps = join(st.path, 'feature/TPSim_3groups_Enc/TPS_btw_thgh_time_v=1_elecs=all/')\n",
    "tps_form = join(path_tps, 'TPS_pears_learn_{}_btw_{}_{}_2gr.npz')\n",
    "df_name = join(path_tps, '{}_ols_btw_{}_{}.csv') #su, conds0, conds1, freq\n",
    "###############################################################################\n",
    "conds_df = ['low_0','low_1','mid_0','mid_1','high_0','high_1']\n",
    "conds = ['low','mid','high']\n",
    "steps = ['early','late']\n",
    "\n",
    "files = st.search('Bilan_elecs_mem_conds', folder=path_tps)\n",
    "pvals = []\n",
    "for fi in files:\n",
    "    df = pd.read_csv(path_tps+fi)\n",
    "    #df_gr = df.groupby(['channels']).mean()\n",
    "    df_gr = df\n",
    "    n_su = df_gr.shape[0]\n",
    "    subj_ = np.concatenate([df_gr['subjects'].values]*len(conds_df))[:,np.newaxis]\n",
    "    tps = df_gr[conds_df].values.T.flatten()[:,np.newaxis]\n",
    "    conds_ = np.concatenate([[cond]*(n_su*2) for cond in conds])[:,np.newaxis]\n",
    "    steps_ = np.concatenate([[step]*n_su for step in steps]*len(conds))[:,np.newaxis]\n",
    "    df_stat = pd.DataFrame(data=np.concatenate((subj_,tps,conds_,steps_),axis=1),\n",
    "                  columns=['subjs','tps','conds','steps'])\n",
    "    df_stat['tps'] = df_stat['tps'].astype(float)\n",
    "    print(df_stat.loc[df_stat['subjs']=='PIRJ'].groupby(['steps','conds']).mean())\n",
    "    df_stat = df_stat.groupby(['subjs','steps']).mean()\n",
    "    df_stat = df_stat.reset_index()\n",
    "    print(df_stat)\n",
    "    #md = smf.mixedlm(\"tps ~ steps\", df_stat, groups=df_stat[\"subjs\"])\n",
    "    #mdf = md.fit()\n",
    "    #print(mdf.summary())\n",
    "    anova = pg.rm_anova(dv='tps',within=['steps'],\n",
    "                  subject='subjs', data=df_stat)\n",
    "    pvals.append(anova['p-unc'][0])\n",
    "    print(anova)\n",
    "    cols0 = [col for col in df_gr[conds_df] if col.endswith('0')]\n",
    "    cols1 = [col for col in df_gr[conds_df] if col.endswith('1')]\n",
    "    means0, means1 = df_gr[cols0].values.mean(), df_gr[cols1].values.mean()\n",
    "    sem0, sem1 = sem(np.ravel(df_gr[cols0].values)), sem(np.ravel(df_gr[cols1].values))\n",
    "    ax = plt.bar(np.arange(2),np.array([means0,means1]), yerr=np.array([sem0,sem1]))\n",
    "    print(fi)    \n",
    "    #ax = df[conds_df].mean().plot.bar(yerr=df[conds_df].sem())\n",
    "    plt.savefig(path_tps+fi.replace('.csv','.png').replace('Bilan','Fig'))\n",
    "    plt.savefig(path_tps+fi.replace('.csv','.pdf').replace('Bilan','Fig'))\n",
    "    plt.clf()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Olfacto loaded\n",
      "low_f\n",
      "high_f\n",
      "after selection for CHAF fam early [(3, 22), (3, 16)]\n",
      "low_f\n",
      "high_f\n",
      "after selection for LEFC fam early [(5, 190), (5, 366)]\n",
      "low_f\n",
      "high_f\n",
      "after selection for PIRJ fam early [(4, 84), (4, 60)]\n",
      "low_f\n",
      "high_f\n",
      "after selection for VACJ fam early [(4, 38), (4, 32)]\n",
      "low_f\n",
      "high_f\n",
      "after selection for SEMC fam early [(3, 172), (3, 98)]\n",
      "low_f\n",
      "high_f\n",
      "after selection for FERJ fam early [(0, 68), (0, 72)]\n",
      "(19, 14)\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for CHAF pleas early [(3, 16), (3, 22)]\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for LEFC pleas early [(5, 286), (5, 270)]\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for PIRJ pleas early [(4, 60), (4, 84)]\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for VACJ pleas early [(4, 32), (4, 38)]\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for SEMC pleas early [(3, 96), (3, 174)]\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for FERJ pleas early [(0, 82), (0, 58)]\n",
      "(19, 14)\n",
      "low_f\n",
      "high_f\n",
      "after selection for CHAF fam late [(3, 60), (3, 36)]\n",
      "low_f\n",
      "high_f\n",
      "after selection for LEFC fam late [(5, 214), (5, 484)]\n",
      "low_f\n",
      "high_f\n",
      "after selection for PIRJ fam late [(4, 108), (4, 82)]\n",
      "low_f\n",
      "high_f\n",
      "after selection for VACJ fam late [(4, 78), (4, 68)]\n",
      "low_f\n",
      "high_f\n",
      "after selection for SEMC fam late [(3, 254), (3, 160)]\n",
      "low_f\n",
      "high_f\n",
      "after selection for FERJ fam late [(0, 128), (0, 146)]\n",
      "(19, 14)\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for CHAF pleas late [(3, 42), (3, 54)]\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for LEFC pleas late [(5, 350), (5, 348)]\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for PIRJ pleas late [(4, 82), (4, 108)]\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for VACJ pleas late [(4, 68), (4, 78)]\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for SEMC pleas late [(3, 177), (3, 237)]\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for FERJ pleas late [(0, 161), (0, 113)]\n",
      "(19, 14)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem, ttest_ind\n",
    "import numpy as np\n",
    "from utils import d_su_gr_fam_pl, rename_elecs\n",
    "from brainpipe.system import study\n",
    "from os.path import join\n",
    "from itertools import product\n",
    "from mne.stats import bonferroni_correction, fdr_correction\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "TPD Early/Late as a function of Memory Richness, Pleasantness or Familiarity\n",
    "\"\"\"\n",
    "freq = 'theta' #'1_alpha', '2_beta','3_gamma', high_gamma\n",
    "steps = ['early','late']\n",
    "feats = {'fam':['low_f','high_f'],\n",
    "        'pleas':['neu_pl', 'ext_pl']}\n",
    "rois_sel = ['OFC_olf']\n",
    "\n",
    "st = study('Olfacto')\n",
    "###############################################################################\n",
    "path_tps = join(st.path, 'feature/TPSim_3groups_Enc/by_odor_btw_thgh_time_v=1_elecs=all/')\n",
    "tps_form = join(path_tps, 'TPS_pears_learn_{}_btw_{}_{}_2gr.npz') #su, od, freq\n",
    "save_path = join(st.path, 'feature/TPSim_3groups_Enc/Ttests_early_late_fam_pl_TPD/')\n",
    "df_name = join(save_path, '{}_Ttests_{}_{}.csv') \n",
    "###############################################################################\n",
    "\n",
    "for step, feat in product(steps,feats):\n",
    "    subjects_c, labels_c = np.array([]), np.array([])\n",
    "    channels_c, x_c, y_c, z_c = np.array([]), np.array([]), np.array([]), np.array([])\n",
    "    tps0_c, tps1_c, sem0_c, sem1_c = np.array([]), np.array([]), np.array([]), np.array([])\n",
    "    T_vals, p_vals = np.array([]), np.array([])\n",
    "    \n",
    "    for su in d_su_gr_fam_pl:\n",
    "        conds = []\n",
    "        for cond in feats[feat]:\n",
    "            print(cond)\n",
    "            btw_tps = np.array([])\n",
    "            for od in d_su_gr_fam_pl[su][cond]:\n",
    "                mat = np.load(tps_form.format(su,od,freq),allow_pickle=True)\n",
    "                data = mat['tps_0'] if step == 'early' else mat['tps_1']\n",
    "                btw_tps = np.concatenate((btw_tps,data),axis=1) if np.size(btw_tps) else data\n",
    "            conds.append(btw_tps)\n",
    "        \n",
    "        #select electrodes\n",
    "        labels, channels = mat['label'], mat['channel']\n",
    "        x, y, z = mat['xyz'][:,0], mat['xyz'][:,1], mat['xyz'][:,2]\n",
    "        labels_new = rename_elecs(labels,x,y,z)\n",
    "        idx_sel = [i for i,lab in enumerate(labels_new) if lab in rois_sel]\n",
    "        nelecs = len(idx_sel)\n",
    "        labels, channels = labels_new[idx_sel], channels[idx_sel]\n",
    "        x, y, z = x[idx_sel], y[idx_sel], z[idx_sel]\n",
    "        sel_data = [1 - x[idx_sel,:] for x in conds]\n",
    "        print('after selection for',su,feat,step,[x.shape for x in sel_data])\n",
    "        \n",
    "        if sel_data[0].shape[0] > 0:\n",
    "            #compute stats (unpaired Ttests)\n",
    "            Tvals_, unc_p = ttest_ind(sel_data[0],sel_data[1],axis=1)\n",
    "            tps0, tps1 = np.mean(sel_data[0],axis=1), np.mean(sel_data[1],axis=1)\n",
    "            sem0, sem1 = sem(sel_data[0],axis=1), sem(sel_data[1],axis=1)\n",
    "\n",
    "            #concatenate all data in order to create the df\n",
    "            subjects_c = np.hstack((subjects_c,np.array([su]*nelecs))) if np.size(subjects_c) else np.array([su]*nelecs)\n",
    "            labels_c = np.hstack((labels_c,labels)) if np.size(labels_c) else labels\n",
    "            channels_c = np.hstack((channels_c,channels)) if np.size(channels_c) else channels\n",
    "            x_c = np.hstack((x_c,x)) if np.size(x_c) else x\n",
    "            y_c = np.hstack((y_c,y)) if np.size(y_c) else y\n",
    "            z_c = np.hstack((z_c,z)) if np.size(z_c) else z\n",
    "\n",
    "            #add stats and descriptive values (sem, mean)\n",
    "            tps0_c = np.hstack((tps0_c,tps0)) if np.size(tps0_c) else tps0\n",
    "            tps1_c = np.hstack((tps1_c,tps1)) if np.size(tps1_c) else tps1\n",
    "            sem0_c = np.hstack((sem0_c,sem0)) if np.size(sem0_c) else sem0\n",
    "            sem1_c = np.hstack((sem1_c,sem1)) if np.size(sem1_c) else sem1\n",
    "            T_vals = np.hstack((T_vals,Tvals_)) if np.size(T_vals) else Tvals_\n",
    "            p_vals = np.hstack((p_vals,unc_p)) if np.size(p_vals) else unc_p\n",
    "    _, p_fdr_ = fdr_correction(p_vals)\n",
    "    _, p_bf_ = bonferroni_correction(p_vals)\n",
    "\n",
    "    data = np.concatenate((subjects_c[:,np.newaxis],labels_c[:,np.newaxis],\n",
    "                channels_c[:,np.newaxis],x_c[:,np.newaxis],y_c[:,np.newaxis],\n",
    "                z_c[:,np.newaxis],tps0_c[:,np.newaxis], tps1_c[:,np.newaxis], \n",
    "                sem0_c[:,np.newaxis],sem1_c[:,np.newaxis],\n",
    "                T_vals[:,np.newaxis],p_vals[:,np.newaxis],\n",
    "                p_fdr_[:,np.newaxis],p_bf_[:,np.newaxis]), axis=1)\n",
    "    df = pd.DataFrame(data, columns=['subjects','labels','channels','x','y','z',\n",
    "                'tps_'+feats[feat][0], 'tps_'+feats[feat][1], \n",
    "                'tps_sem_'+feats[feat][0], 'tps_sem_'+feats[feat][1], \n",
    "                'Tvals','unc_p','fdr_p','bonf_p'])\n",
    "    print(df.shape)\n",
    "    df.to_csv(df_name.format('All_subjects',feat,step+'_'+freq),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Olfacto loaded\n",
      "low_f\n",
      "high_f\n",
      "after selection for CHAF fam late [(3, 60), (3, 36)]\n",
      "low_f\n",
      "high_f\n",
      "after selection for LEFC fam late [(5, 214), (5, 484)]\n",
      "low_f\n",
      "high_f\n",
      "after selection for PIRJ fam late [(4, 108), (4, 82)]\n",
      "low_f\n",
      "high_f\n",
      "after selection for VACJ fam late [(4, 78), (4, 68)]\n",
      "low_f\n",
      "high_f\n",
      "after selection for SEMC fam late [(3, 254), (3, 160)]\n",
      "low_f\n",
      "high_f\n",
      "after selection for FERJ fam late [(0, 128), (0, 146)]\n",
      "(19, 14)\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for CHAF pleas late [(3, 42), (3, 54)]\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for LEFC pleas late [(5, 350), (5, 348)]\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for PIRJ pleas late [(4, 82), (4, 108)]\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for VACJ pleas late [(4, 68), (4, 78)]\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for SEMC pleas late [(3, 177), (3, 237)]\n",
      "neu_pl\n",
      "ext_pl\n",
      "after selection for FERJ pleas late [(0, 161), (0, 113)]\n",
      "(19, 14)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem, ttest_ind\n",
    "import numpy as np\n",
    "from utils import d_su_gr_fam_pl, rename_elecs\n",
    "from brainpipe.system import study\n",
    "from os.path import join\n",
    "from itertools import product\n",
    "from mne.stats import bonferroni_correction, fdr_correction\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "Delta TPD Early/Late as a function of Memory Richness, Pleasantness or Familiarity\n",
    "\"\"\"\n",
    "freq = 'theta' #'1_alpha', '2_beta','3_gamma', high_gamma\n",
    "feats = {'fam':['low_f','high_f'],\n",
    "        'pleas':['neu_pl', 'ext_pl']}\n",
    "rois_sel = ['OFC_olf']\n",
    "\n",
    "st = study('Olfacto')\n",
    "###############################################################################\n",
    "path_tps = join(st.path, 'feature/TPSim_3groups_Enc/by_odor_btw_thgh_time_v=1_elecs=all/')\n",
    "tps_form = join(path_tps, 'TPS_pears_learn_{}_btw_{}_{}_2gr.npz') #su, od, freq\n",
    "save_path = join(st.path, 'feature/TPSim_3groups_Enc/Ttests_early_late_fam_pl_TPD/')\n",
    "df_name = join(save_path, '{}_Ttests_{}_{}.csv') \n",
    "###############################################################################\n",
    "\n",
    "for feat in feats:\n",
    "    subjects_c, labels_c = np.array([]), np.array([])\n",
    "    channels_c, x_c, y_c, z_c = np.array([]), np.array([]), np.array([]), np.array([])\n",
    "    tps0_c, tps1_c, sem0_c, sem1_c = np.array([]), np.array([]), np.array([]), np.array([])\n",
    "    T_vals, p_vals = np.array([]), np.array([])\n",
    "    \n",
    "    for su in d_su_gr_fam_pl:\n",
    "        conds0, conds1 = [], []\n",
    "        for cond in feats[feat]:\n",
    "            print(cond)\n",
    "            btw_tps0, btw_tps1 = np.array([]), np.array([])\n",
    "            for od in d_su_gr_fam_pl[su][cond]:\n",
    "                mat = np.load(tps_form.format(su,od,freq),allow_pickle=True)\n",
    "                data0, data1 = mat['tps_0'], mat['tps_1']\n",
    "                btw_tps0 = np.concatenate((btw_tps0,data0),axis=1) if np.size(btw_tps0) else data0\n",
    "                btw_tps1 = np.concatenate((btw_tps1,data1),axis=1) if np.size(btw_tps1) else data1\n",
    "            conds0.append(btw_tps0), conds1.append(btw_tps1)\n",
    "        \n",
    "        #select electrodes\n",
    "        labels, channels = mat['label'], mat['channel']\n",
    "        x, y, z = mat['xyz'][:,0], mat['xyz'][:,1], mat['xyz'][:,2]\n",
    "        labels_new = rename_elecs(labels,x,y,z)\n",
    "        idx_sel = [i for i,lab in enumerate(labels_new) if lab in rois_sel]\n",
    "        nelecs = len(idx_sel)\n",
    "        labels, channels = labels_new[idx_sel], channels[idx_sel]\n",
    "        x, y, z = x[idx_sel], y[idx_sel], z[idx_sel]\n",
    "        sel_data0 = [1 - x[idx_sel,:] for x in conds0]\n",
    "        sel_data1 = [1 - x[idx_sel,:] for x in conds1]\n",
    "        print('after selection for',su,feat,step,[x.shape for x in sel_data1])\n",
    "        \n",
    "        #create the diff of TPD\n",
    "        diff_data_l, diff_data_h = np.array([]), np.array([])\n",
    "        for elec in range(nelecs):\n",
    "            d0_l, d1_l = sel_data0[0][elec], sel_data1[0][elec]\n",
    "            d0_h, d1_h = sel_data0[1][elec], sel_data1[1][elec]\n",
    "            diff_l = np.array([y-x for x,y in product(d0_l,d1_l)])[np.newaxis]\n",
    "            diff_h = np.array([y-x for x,y in product(d0_h,d1_h)])[np.newaxis]\n",
    "            diff_data_l = np.concatenate((diff_data_l,diff_l),axis=0) if np.size(diff_data_l) else diff_l\n",
    "            diff_data_h = np.concatenate((diff_data_h,diff_h),axis=0) if np.size(diff_data_h) else diff_h\n",
    "        diff_all = [diff_data_l, diff_data_h]\n",
    "        \n",
    "        if diff_all[0].shape[0] > 0:\n",
    "            #compute stats (unpaired Ttests)\n",
    "            Tvals_, unc_p = ttest_ind(diff_all[0],diff_all[1],axis=1)\n",
    "            tps0, tps1 = np.mean(diff_all[0],axis=1), np.mean(diff_all[1],axis=1)\n",
    "            sem0, sem1 = sem(diff_all[0],axis=1), sem(diff_all[1],axis=1)\n",
    "\n",
    "            #concatenate all data in order to create the df\n",
    "            subjects_c = np.hstack((subjects_c,np.array([su]*nelecs))) if np.size(subjects_c) else np.array([su]*nelecs)\n",
    "            labels_c = np.hstack((labels_c,labels)) if np.size(labels_c) else labels\n",
    "            channels_c = np.hstack((channels_c,channels)) if np.size(channels_c) else channels\n",
    "            x_c = np.hstack((x_c,x)) if np.size(x_c) else x\n",
    "            y_c = np.hstack((y_c,y)) if np.size(y_c) else y\n",
    "            z_c = np.hstack((z_c,z)) if np.size(z_c) else z\n",
    "\n",
    "            #add stats and descriptive values (sem, mean)\n",
    "            tps0_c = np.hstack((tps0_c,tps0)) if np.size(tps0_c) else tps0\n",
    "            tps1_c = np.hstack((tps1_c,tps1)) if np.size(tps1_c) else tps1\n",
    "            sem0_c = np.hstack((sem0_c,sem0)) if np.size(sem0_c) else sem0\n",
    "            sem1_c = np.hstack((sem1_c,sem1)) if np.size(sem1_c) else sem1\n",
    "            T_vals = np.hstack((T_vals,Tvals_)) if np.size(T_vals) else Tvals_\n",
    "            p_vals = np.hstack((p_vals,unc_p)) if np.size(p_vals) else unc_p\n",
    "    _, p_fdr_ = fdr_correction(p_vals)\n",
    "    _, p_bf_ = bonferroni_correction(p_vals)\n",
    "\n",
    "    data = np.concatenate((subjects_c[:,np.newaxis],labels_c[:,np.newaxis],\n",
    "                channels_c[:,np.newaxis],x_c[:,np.newaxis],y_c[:,np.newaxis],\n",
    "                z_c[:,np.newaxis],tps0_c[:,np.newaxis], tps1_c[:,np.newaxis], \n",
    "                sem0_c[:,np.newaxis],sem1_c[:,np.newaxis],\n",
    "                T_vals[:,np.newaxis],p_vals[:,np.newaxis],\n",
    "                p_fdr_[:,np.newaxis],p_bf_[:,np.newaxis]), axis=1)\n",
    "    df = pd.DataFrame(data, columns=['subjects','labels','channels','x','y','z',\n",
    "                'tps_'+feats[feat][0], 'tps_'+feats[feat][1], \n",
    "                'tps_sem_'+feats[feat][0], 'tps_sem_'+feats[feat][1], \n",
    "                'Tvals','unc_p','fdr_p','bonf_p'])\n",
    "    print(df.shape)\n",
    "    df.to_csv(df_name.format('All_subjects',feat,'deltaTPD_'+freq),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
