{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter, MaxNLocator\n",
    "import scipy.io as sio\n",
    "\n",
    "from brainpipe.classification import *\n",
    "from brainpipe.system import study\n",
    "from brainpipe.feature import power, amplitude, sigfilt\n",
    "from brainpipe.visual import *\n",
    "from brainpipe.statistics import *\n",
    "from scipy.stats import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Decoding - Partial//Detailed Encoding\n",
    "### For ALL time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import StratifiedKFold as SKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from numpy.random import permutation\n",
    "st = study('Olfacto')\n",
    "conds, subjects = ['low','high'],['CHAF','VACJ','SEMC','LEFC','PIRJ','FERJ']\n",
    "freqs = ['0_theta','1_alpha','2_beta','3_gamma']\n",
    "path_pow = path.join(st.path, 'feature/TPSim_Enc_By_Odor_By_Cond/TPS_by_odor/')\n",
    "save_path = path.join(st.path, 'classified/Classif_TPSim_E_by_odor/')\n",
    "nperm = 1000\n",
    "\n",
    "for su in subjects:\n",
    "    for freq in freqs:\n",
    "        pow_list = []\n",
    "        #=========================== Load Power files (nfreq, nelec, nwin, ntrial) =================================    \n",
    "        mat0 = np.load(path.join(path_pow, 'TPS_spear_'+su+'_odor_'+conds[0]+'_'+freq+'.npz'),\n",
    "                       allow_pickle=True)\n",
    "        names = mat0['label']\n",
    "        pow_list.append(mat0['tps'])\n",
    "        nelecs = mat0['tps'].shape[0]\n",
    "        mat1 = np.load(path.join(path_pow, 'TPS_spear_'+su+'_odor_'+conds[1]+'_'+freq+'.npz'),\n",
    "                       allow_pickle=True)\n",
    "        pow_list.append(mat1['tps'])\n",
    "        print (su, 'TPS shape: ', [pow.shape for pow in pow_list])\n",
    "        # =========================== Select Power for 1 elec 1 freq =================================                 \n",
    "        iterator = range(nelecs)\n",
    "        for elec_num in iterator:#iterator\n",
    "            elec_label = names[elec_num]\n",
    "            print ('elec ', elec_num, 'elec_label ', elec_label)\n",
    "            #Filenames to save\n",
    "            name_auc = (save_path+freq+'/'+su +'_auc_'+conds[0]+'_'+conds[1]+'_'+str(elec_label)+'_('+str(elec_num)+').npy')\n",
    "            name_perm = (save_path+freq+'/'+su +'_perm_'+str(elec_label)+'_('+str(elec_num)+').npy')\n",
    "                        \n",
    "            if path.exists(name_auc):\n",
    "                print(su,phase,elec_num,freq,'already computed')\n",
    "            else:\n",
    "                print('--Â» processing',su, 'elec', elec_num,'/',nelecs, 'freq',freq)\n",
    "                pow_data_elec = []\n",
    "                for i,power in enumerate(pow_list):\n",
    "                    pow_data_elec.append(power[elec_num][:,np.newaxis])\n",
    "        # =============================  Classification Computation ============================================================           \n",
    "                # create a data matrix, concatenate along the trial dimension\n",
    "                x = np.concatenate(pow_data_elec, axis=0)\n",
    "                print ('Size of the concatenated data: ', x.shape, 'Number time windows : ', x.shape[1])\n",
    "                y = np.hstack([np.array([i]*len(power)) for i, power in enumerate(pow_data_elec)])\n",
    "                print ('Size of label for classif: ', len(y))\n",
    "\n",
    "                auc = np.array([])\n",
    "                for t in range(x.shape[1]):\n",
    "                    X = x[:,t]\n",
    "                    X = X.reshape(-1, 1)\n",
    "                    score_rep = []\n",
    "                    for i in range(10):\n",
    "                        k = 5\n",
    "                        skf = SKFold(n_splits=k, random_state=None, shuffle=True)\n",
    "                        skf.get_n_splits(X, y)\n",
    "                        score_cv = []\n",
    "                        for train_index, test_index in skf.split(X, y):\n",
    "                            clf = LDA()\n",
    "                            X_train, X_test = X[train_index], X[test_index]\n",
    "                            y_train, y_test = y[train_index], y[test_index]\n",
    "                            clf.fit(X=X_train, y=y_train)\n",
    "                            y_pred = clf.predict(X_test)\n",
    "                            score_cv.append(roc_auc_score(y_test,y_pred,average='weighted'))\n",
    "                        score_rep.append(np.mean(score_cv))\n",
    "                    score_rep = np.asarray(score_rep).reshape(1,len(score_rep))\n",
    "                    auc = np.vstack((auc, score_rep)) if np.size(auc) else score_rep\n",
    "                auc = np.swapaxes(auc,0,1)\n",
    "\n",
    "                perm_scores = np.array([])\n",
    "                for t in range(x.shape[1]):\n",
    "                    X = x[:,t]\n",
    "                    X = X.reshape(-1, 1)\n",
    "                    perm_rep = []\n",
    "                    for perm in range(nperm):\n",
    "                        y_perm = y[permutation(len(y))]\n",
    "                        score_cv = []\n",
    "                        for train_index, test_index in skf.split(X, y_perm):\n",
    "                            clf = LDA()\n",
    "                            X_train, X_test = X[train_index], X[test_index]\n",
    "                            y_train, y_test = y_perm[train_index], y_perm[test_index]\n",
    "                            clf.fit(X=X_train, y=y_train)\n",
    "                            y_pred = clf.predict(X_test)\n",
    "                            score_cv.append(roc_auc_score(y_test,y_pred,average='weighted'))\n",
    "                        perm_rep.append(np.mean(score_cv))\n",
    "                    perm_rep = np.asarray(perm_rep).reshape(1,len(perm_rep))\n",
    "                    perm_scores = np.vstack((perm_scores, perm_rep)) if np.size(perm_scores) else perm_rep\n",
    "                perm_scores = np.swapaxes(perm_scores,0,1)           \n",
    "                th_0_05_perm = perm_pvalue2level(perm_scores, p=0.005, maxst=True)\n",
    "                th_0_01_perm = perm_pvalue2level(perm_scores, p=0.001, maxst=True)\n",
    "                print('th_perm 005: ', th_0_05_perm[0], '001',th_0_01_perm[0], 'auc_max', np.max(auc))\n",
    "\n",
    "                #Save plots\n",
    "                np.save(name_auc, auc)\n",
    "                np.save(name_perm, perm_scores)\n",
    "                del X, auc, pow_data_elec\n",
    "    del pow_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
